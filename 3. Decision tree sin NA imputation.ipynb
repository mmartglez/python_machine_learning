{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decision Tree\n",
    "\n",
    "Realicemos una predicción basada en un random forest. \n",
    "Se parte de los datos analizados, normalizados y acotados logrados en el punto 0, para el training.\n",
    "\n",
    "Este método se basa en hacer varios decision trees. Veremos que necesita ajustes y no suele ser el mejor método para overfitting.\n",
    "\n",
    "Partiendo de una contrucción del modelo, haremos un proceso iterativo de validación y ajuste del mismo (modificando parámetros y variables), hasta obtener el que mejor predice nuestra target, sin infra o sobreajustes\n",
    "\n",
    "## Importación de datos y selección de variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>728.805765</td>\n",
       "      <td>729.805765</td>\n",
       "      <td>56.877145</td>\n",
       "      <td>10460.434454</td>\n",
       "      <td>6.094715</td>\n",
       "      <td>5.576527</td>\n",
       "      <td>1971.194235</td>\n",
       "      <td>1984.818806</td>\n",
       "      <td>439.128346</td>\n",
       "      <td>46.645161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.082361</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.868909</td>\n",
       "      <td>0.069321</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.821551</td>\n",
       "      <td>0.084420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.402158</td>\n",
       "      <td>421.402158</td>\n",
       "      <td>42.339638</td>\n",
       "      <td>9862.564977</td>\n",
       "      <td>1.376542</td>\n",
       "      <td>1.113638</td>\n",
       "      <td>30.190353</td>\n",
       "      <td>20.640669</td>\n",
       "      <td>432.964939</td>\n",
       "      <td>161.471529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.275008</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.337616</td>\n",
       "      <td>0.254086</td>\n",
       "      <td>0.052342</td>\n",
       "      <td>0.090410</td>\n",
       "      <td>0.116395</td>\n",
       "      <td>0.383022</td>\n",
       "      <td>0.278112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>364.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7540.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>729.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9473.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1093.000000</td>\n",
       "      <td>1094.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>11600.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2188.000000</td>\n",
       "      <td>1474.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           Id   MSSubClass        LotArea  OverallQual  \\\n",
       "count  1457.000000  1457.000000  1457.000000    1457.000000  1457.000000   \n",
       "mean    728.805765   729.805765    56.877145   10460.434454     6.094715   \n",
       "std     421.402158   421.402158    42.339638    9862.564977     1.376542   \n",
       "min       0.000000     1.000000    20.000000    1300.000000     1.000000   \n",
       "25%     364.000000   365.000000    20.000000    7540.000000     5.000000   \n",
       "50%     729.000000   730.000000    50.000000    9473.000000     6.000000   \n",
       "75%    1093.000000  1094.000000    70.000000   11600.000000     7.000000   \n",
       "max    1459.000000  1460.000000   190.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   BsmtFinSF1   BsmtFinSF2  \\\n",
       "count  1457.000000  1457.000000   1457.000000  1457.000000  1457.000000   \n",
       "mean      5.576527  1971.194235   1984.818806   439.128346    46.645161   \n",
       "std       1.113638    30.190353     20.640669   432.964939   161.471529   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1972.000000   1994.000000   383.000000     0.000000   \n",
       "75%       6.000000  2000.000000   2004.000000   712.000000     0.000000   \n",
       "max       9.000000  2010.000000   2010.000000  2188.000000  1474.000000   \n",
       "\n",
       "               ...            SaleType_ConLw  SaleType_New  SaleType_Oth  \\\n",
       "count          ...               1457.000000   1457.000000   1457.000000   \n",
       "mean           ...                  0.003432      0.082361      0.002059   \n",
       "std            ...                  0.058500      0.275008      0.045345   \n",
       "min            ...                  0.000000      0.000000      0.000000   \n",
       "25%            ...                  0.000000      0.000000      0.000000   \n",
       "50%            ...                  0.000000      0.000000      0.000000   \n",
       "75%            ...                  0.000000      0.000000      0.000000   \n",
       "max            ...                  1.000000      1.000000      1.000000   \n",
       "\n",
       "       SaleType_WD  SaleCondition_Abnorml  SaleCondition_AdjLand  \\\n",
       "count  1457.000000            1457.000000            1457.000000   \n",
       "mean      0.868909               0.069321               0.002745   \n",
       "std       0.337616               0.254086               0.052342   \n",
       "min       0.000000               0.000000               0.000000   \n",
       "25%       1.000000               0.000000               0.000000   \n",
       "50%       1.000000               0.000000               0.000000   \n",
       "75%       1.000000               0.000000               0.000000   \n",
       "max       1.000000               1.000000               1.000000   \n",
       "\n",
       "       SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "count           1457.000000           1457.000000           1457.000000   \n",
       "mean               0.008236              0.013727              0.821551   \n",
       "std                0.090410              0.116395              0.383022   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%                0.000000              0.000000              1.000000   \n",
       "50%                0.000000              0.000000              1.000000   \n",
       "75%                0.000000              0.000000              1.000000   \n",
       "max                1.000000              1.000000              1.000000   \n",
       "\n",
       "       SaleCondition_Partial  \n",
       "count            1457.000000  \n",
       "mean                0.084420  \n",
       "std                 0.278112  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 0.000000  \n",
       "75%                 0.000000  \n",
       "max                 1.000000  \n",
       "\n",
       "[8 rows x 222 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Librerías a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importación de datos\n",
    "melbourne_data = pd.read_csv(\"data/PreciosCasas/train_final.csv\", sep='\\t', encoding='utf-8') \n",
    "\n",
    "# print a summary of the data in Melbourne data\n",
    "melbourne_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Id', 'MSSubClass', 'LotArea', 'OverallQual',\n",
      "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       ...\n",
      "       'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD',\n",
      "       'SaleCondition_Abnorml', 'SaleCondition_AdjLand',\n",
      "       'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal',\n",
      "       'SaleCondition_Partial'],\n",
      "      dtype='object', length=222)\n"
     ]
    }
   ],
   "source": [
    "#Vamos a ver que variables elegimos\n",
    "\n",
    "print(melbourne_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos prededir el precio, será nuestro target, para lo cual, cogeremos unas variables como pedictores ( de momento, las que vemos que probablemente mejor predicen la target, y en el proceso de iteración es cuando vamos verficándolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    12.247694\n",
      "1    12.109011\n",
      "2    12.317167\n",
      "3    11.849398\n",
      "4    12.429216\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10460.434454</td>\n",
       "      <td>1971.194235</td>\n",
       "      <td>1159.129032</td>\n",
       "      <td>345.560055</td>\n",
       "      <td>1.563487</td>\n",
       "      <td>2.866163</td>\n",
       "      <td>6.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9862.564977</td>\n",
       "      <td>30.190353</td>\n",
       "      <td>372.015864</td>\n",
       "      <td>435.505117</td>\n",
       "      <td>0.549961</td>\n",
       "      <td>0.816595</td>\n",
       "      <td>1.616384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7540.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9473.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11600.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1391.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215245.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>3228.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LotArea    YearBuilt     1stFlrSF     2ndFlrSF     FullBath  \\\n",
       "count    1457.000000  1457.000000  1457.000000  1457.000000  1457.000000   \n",
       "mean    10460.434454  1971.194235  1159.129032   345.560055     1.563487   \n",
       "std      9862.564977    30.190353   372.015864   435.505117     0.549961   \n",
       "min      1300.000000  1872.000000   334.000000     0.000000     0.000000   \n",
       "25%      7540.000000  1954.000000   882.000000     0.000000     1.000000   \n",
       "50%      9473.000000  1972.000000  1086.000000     0.000000     2.000000   \n",
       "75%     11600.000000  2000.000000  1391.000000   728.000000     2.000000   \n",
       "max    215245.000000  2010.000000  3228.000000  2065.000000     3.000000   \n",
       "\n",
       "       BedroomAbvGr  TotRmsAbvGrd  \n",
       "count   1457.000000   1457.000000  \n",
       "mean       2.866163      6.510638  \n",
       "std        0.816595      1.616384  \n",
       "min        0.000000      2.000000  \n",
       "25%        2.000000      5.000000  \n",
       "50%        3.000000      6.000000  \n",
       "75%        3.000000      7.000000  \n",
       "max        8.000000     14.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= melbourne_data.SalePrice\n",
    "print(y.head())\n",
    "melbourne_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = melbourne_data[melbourne_predictors]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del modelo Decision Tree models\n",
    "\n",
    "El modelo que vamos a hacer es scikit-learn, y luego validamos que tal es mediante Mean Absolute Error (MAE). El error de predicción será: error= valor actual−predicción\n",
    "\n",
    "Para ser capaces de ir validando el modelo, lo separaremos en dos grupos, predictors and target. Lo haremos mediando un split con un número generaro aleatorio. Como queremos que todas las veces que ejecutemos el modelo nos salga lo mismo, estableceremos el argumento de random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "\n",
    "from  sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Separamos los datos en dos grupos, \n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué hacemos esa separación del df en dos grupos?**\n",
    "\n",
    "En este punto, solo por practicar, veamos que pasa tanto si realizo esa separación como si no la hago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_training= test. Predicciones de estas casas: \n",
      "   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "0     8450       2003       856       854         2             3   \n",
      "1     9600       1976      1262         0         2             3   \n",
      "2    11250       2001       920       866         2             3   \n",
      "3     9550       1915       961       756         1             3   \n",
      "4    14260       2000      1145      1053         2             4   \n",
      "\n",
      "   TotRmsAbvGrd  \n",
      "0             8  \n",
      "1             6  \n",
      "2             6  \n",
      "3             7  \n",
      "4             9  \n",
      "La predictions \n",
      "[ 12.24769432  12.10901093  12.31716669  11.8493977   12.4292162 ]\n",
      "y el real es\n",
      "0    12.247694\n",
      "1    12.109011\n",
      "2    12.317167\n",
      "3    11.849398\n",
      "4    12.429216\n",
      "Name: SalePrice, dtype: float64\n",
      "error para el conjunto de training es: \n",
      "0.000492273525725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1_ Si no cogieramos el training y validación: Hacemos el modelo\n",
    "melbourne_model= DecisionTreeRegressor()\n",
    "melbourne_model.fit(X,y)\n",
    "\n",
    "#Veamos como funciona\n",
    "print(\"1_training= test. Predicciones de estas casas: \")\n",
    "print(X.head())\n",
    "print(\"La predictions \")\n",
    "print(melbourne_model.predict(X.head()))\n",
    "print(\"y el real es\")\n",
    "print(y.head())\n",
    "\n",
    "val_predicted_home_prices = melbourne_model.predict(X)\n",
    "print(\"error para el conjunto de training es: \")\n",
    "print (mean_absolute_error(y,val_predicted_home_prices ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_ Training<> Test. Predicciones de estas casas:\n",
      "     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "411    34650       1955      1056         0         1             3   \n",
      "211    10420       2009      1212         0         2             3   \n",
      "342     8544       1949      1040         0         2             2   \n",
      "303     9800       1972       894         0         1             3   \n",
      "159    19378       2005      1392      1070         2             4   \n",
      "\n",
      "     TotRmsAbvGrd  \n",
      "411             5  \n",
      "211             6  \n",
      "342             6  \n",
      "303             5  \n",
      "159             9  \n",
      "The predictions are\n",
      "[ 12.10071213  12.20055746  11.44249782  11.77528973  12.77705219]\n",
      "And the real is\n",
      "411    11.884489\n",
      "211    12.133502\n",
      "342    11.379394\n",
      "303    11.917724\n",
      "159    12.676076\n",
      "Name: SalePrice, dtype: float64\n",
      "y en este caso el error es: \n",
      "0.169929723949\n"
     ]
    }
   ],
   "source": [
    "#2_ Si cogemos el training y validación: Hacemos el modelo\n",
    "melbourne_model= DecisionTreeRegressor()\n",
    "melbourne_model.fit(train_X,train_y)\n",
    "\n",
    "#Veamos como funciona\n",
    "print(\"2_ Training<> Test. Predicciones de estas casas:\")\n",
    "print(val_X.head())\n",
    "print(\"The predictions are\")\n",
    "print(melbourne_model.predict(val_X.head()))\n",
    "print(\"And the real is\")\n",
    "print(val_y.head())\n",
    "\n",
    "#Error cometido en esta medicion MAE \n",
    "val_predicted_home_prices2 = melbourne_model.predict(val_X)\n",
    "print(\"y en este caso el error es: \")\n",
    "print (mean_absolute_error(val_y,val_predicted_home_prices2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njBLDhN8wDKPEMOE3DMMoMf4/JzCJ8f/7nBoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f744fbc3198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veamoslo en un scatter plot\n",
    "plt.scatter(val_predicted_home_prices, val_predicted_home_prices );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYgqhoGfiKhi/j8VUTACOa/ldQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f744ab70748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veamoslo en un scatter plot\n",
    "plt.scatter(val_predicted_home_prices2, val_y );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar el primer modelo se comporta demasiado bien, pero es que está sobreajustado, y luego con el test se comportará mal. Es mejor el segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  35190\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  27825\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  32662\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  33382\n"
     ]
    }
   ],
   "source": [
    "# 3_ Ahora vamos a ajustar mejor el modelo definiendo cuantas ramas tendrá el arbol\n",
    "\n",
    "def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(predictors_val)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model\n",
    "The random forest uses many trees, and it makes a prediction by average the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23781.467032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionar variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "Mean Absolute Error from dropping columns with Missing Values:\n",
      "18055.5090564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#1) Limpiar datos, borrando valores perdidos\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "melb_data= melbourne_data\n",
    "melb_target = melbourne_data.SalePrice\n",
    "melb_predictors = melbourne_data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "\n",
    "#funcion para calcular MAE\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(n_estimators=60, max_depth=10, min_samples_split=10, random_state=29011982)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)\n",
    "\n",
    "\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print (cols_with_missing)\n",
    "print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
       "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
       "       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
       "       'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\n",
       "       'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation:\n",
      "17785.7748086\n"
     ]
    }
   ],
   "source": [
    "#2) Imputation\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "\n",
    "#print(pd.DataFrame(imputed_X_train).sample (5))\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, numpy.ndarray)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(imputed_X_train)  # como son tipos distintos no aplican las mismas propiedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
      "17804.8957325\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = (col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ¿el modelo es estable? hago el cross-validation para un random forest y para un boosting\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'))\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2'])                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20037.52170088  21254.58768328  18538.27235294]\n",
      "[ 0.80786494  0.85106733  0.84485583]\n"
     ]
    }
   ],
   "source": [
    "score_dataset_cv(reduced_X_train, reduced_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17838.61492361  18723.1462148   15069.57012214]\n",
      "[ 0.79428757  0.8755134   0.89312167]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor()\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'))\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2'])                         \n",
    "\n",
    "score_dataset_cv(reduced_X_train, reduced_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [70, 100, 120], 'max_depth': [3, 5, 7, 10], 'subsample': [0.6, 0.7, 0.8], 'min_weight_fraction_leaf': [0.2, 0.1, 0.05]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiperparámetros: mirar que parámetros son los que debo cambiar para mi modelo\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators': [70, 100, 120], 'max_depth': [3, 5, 7, 10], 'subsample': [0.60, 0.7, 0.80],'min_weight_fraction_leaf':[0.20, 0.1, 0.05]}\n",
    "gbr = GradientBoostingRegressor()\n",
    "clf = GridSearchCV(gbr, parameters)\n",
    "clf.fit(reduced_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.11714339,  0.11063226,  0.12684703,  0.1728754 ,  0.17218852,\n",
       "         0.16924175,  0.17997193,  0.18668445,  0.18607386,  0.12267931,\n",
       "         0.13055062,  0.13252672,  0.17671243,  0.17018708,  0.16864181,\n",
       "         0.21145304,  0.20427322,  0.21248802,  0.15325721,  0.15230163,\n",
       "         0.12965608,  0.19285822,  0.18939575,  0.17586199,  0.22160554,\n",
       "         0.21080907,  0.20912997,  0.10416691,  0.12761633,  0.12933501,\n",
       "         0.1494631 ,  0.14823429,  0.15870674,  0.1798652 ,  0.18544054,\n",
       "         0.18467911,  0.1501797 ,  0.1607914 ,  0.17250355,  0.21540658,\n",
       "         0.24059312,  0.2426095 ,  0.27144027,  0.26941133,  0.28824711,\n",
       "         0.18110792,  0.18133903,  0.18775423,  0.24791773,  0.26486731,\n",
       "         0.28458309,  0.29874619,  0.3225166 ,  0.33392493,  0.11015924,\n",
       "         0.10737125,  0.11646843,  0.15099899,  0.15411075,  0.16042129,\n",
       "         0.17211509,  0.17684865,  0.19291862,  0.16465108,  0.16844392,\n",
       "         0.17785048,  0.22965948,  0.26353717,  0.27792478,  0.28878069,\n",
       "         0.29476722,  0.30583556,  0.21441507,  0.22577397,  0.23381424,\n",
       "         0.34391602,  0.33021855,  0.35114225,  0.36516158,  0.38948075,\n",
       "         0.3805186 ,  0.10505954,  0.13249048,  0.10659838,  0.15436101,\n",
       "         0.15138062,  0.15655613,  0.18316062,  0.19992097,  0.18030087,\n",
       "         0.1683201 ,  0.17862733,  0.16732383,  0.2354598 ,  0.24290546,\n",
       "         0.26095899,  0.26981537,  0.29492331,  0.31533702,  0.24293486,\n",
       "         0.25075618,  0.25162387,  0.35722462,  0.35562849,  0.37873658,\n",
       "         0.40811761,  0.4387536 ,  0.44904947]),\n",
       " 'mean_score_time': array([ 0.00134548,  0.00145904,  0.00148217,  0.00166368,  0.00177089,\n",
       "         0.00241216,  0.00215459,  0.00169206,  0.00206288,  0.00165788,\n",
       "         0.00165192,  0.00163658,  0.00176136,  0.00162903,  0.00171423,\n",
       "         0.00292404,  0.0021979 ,  0.00200089,  0.00145419,  0.00166758,\n",
       "         0.00174999,  0.00196195,  0.00153899,  0.00331982,  0.0023307 ,\n",
       "         0.00195638,  0.00186769,  0.00152906,  0.00170414,  0.00157682,\n",
       "         0.00186666,  0.00151634,  0.0017813 ,  0.00215952,  0.00206033,\n",
       "         0.00252867,  0.001851  ,  0.00178361,  0.00388662,  0.00208926,\n",
       "         0.00191037,  0.00355713,  0.00223192,  0.0028158 ,  0.00211692,\n",
       "         0.00206168,  0.00188835,  0.00235446,  0.00225989,  0.00336266,\n",
       "         0.00181492,  0.00271511,  0.00279895,  0.00243497,  0.00141279,\n",
       "         0.00146063,  0.00144807,  0.001779  ,  0.00172758,  0.00221133,\n",
       "         0.00205016,  0.00199564,  0.0018096 ,  0.00186292,  0.00194144,\n",
       "         0.00157348,  0.00216444,  0.00375223,  0.00261815,  0.00369732,\n",
       "         0.00260035,  0.00254011,  0.00239333,  0.00215721,  0.00199421,\n",
       "         0.00251683,  0.00280118,  0.0025657 ,  0.0031182 ,  0.00241335,\n",
       "         0.00277217,  0.00162776,  0.00179116,  0.00171216,  0.00189789,\n",
       "         0.00172289,  0.00348552,  0.00188486,  0.00203657,  0.00179164,\n",
       "         0.00152787,  0.00201734,  0.00169945,  0.00207567,  0.00253987,\n",
       "         0.002225  ,  0.00271265,  0.00212153,  0.00307616,  0.0023694 ,\n",
       "         0.00228039,  0.00212709,  0.00307631,  0.00270001,  0.00262984,\n",
       "         0.00511591,  0.00293008,  0.00302529]),\n",
       " 'mean_test_score': array([ 0.78350144,  0.78457752,  0.77991719,  0.79563271,  0.79477609,\n",
       "         0.79431563,  0.80365568,  0.79909173,  0.80158269,  0.83536301,\n",
       "         0.83572328,  0.8308875 ,  0.83989148,  0.83740334,  0.8415559 ,\n",
       "         0.84340321,  0.84539483,  0.84368825,  0.85172258,  0.8530946 ,\n",
       "         0.85168265,  0.85052992,  0.85091074,  0.84719844,  0.8532906 ,\n",
       "         0.84944614,  0.84816658,  0.78631237,  0.77919019,  0.78022763,\n",
       "         0.79729935,  0.79233056,  0.79513444,  0.8016102 ,  0.80382043,\n",
       "         0.80285868,  0.83868128,  0.8343607 ,  0.83285429,  0.83796568,\n",
       "         0.84124133,  0.8386922 ,  0.84190888,  0.83749431,  0.84078784,\n",
       "         0.8527653 ,  0.85271024,  0.84900645,  0.84986879,  0.84739886,\n",
       "         0.84875916,  0.84828401,  0.85282111,  0.84622147,  0.78948047,\n",
       "         0.78002928,  0.778824  ,  0.79877648,  0.79322155,  0.79856886,\n",
       "         0.79841858,  0.79754873,  0.80301895,  0.83304271,  0.83486067,\n",
       "         0.83082684,  0.84278881,  0.83957634,  0.83675852,  0.8382215 ,\n",
       "         0.83813744,  0.84180764,  0.84704118,  0.84794089,  0.85092953,\n",
       "         0.85127002,  0.8486924 ,  0.84981132,  0.84626913,  0.8531923 ,\n",
       "         0.84261747,  0.785761  ,  0.77970546,  0.77836121,  0.79506055,\n",
       "         0.79401551,  0.79079335,  0.80536035,  0.79832825,  0.80128029,\n",
       "         0.83701425,  0.83667493,  0.83118502,  0.84258555,  0.84082727,\n",
       "         0.83750907,  0.84632181,  0.83943606,  0.84157607,  0.84949842,\n",
       "         0.85007268,  0.84822857,  0.84695395,  0.85201167,  0.85191881,\n",
       "         0.84788386,  0.84559   ,  0.84798434]),\n",
       " 'mean_train_score': array([ 0.80805441,  0.80733199,  0.80507939,  0.82684072,  0.82599484,\n",
       "         0.82530729,  0.84106899,  0.83630293,  0.83631481,  0.87427861,\n",
       "         0.87356369,  0.87149533,  0.88869524,  0.88633807,  0.88858192,\n",
       "         0.90019874,  0.89656112,  0.8978531 ,  0.91414816,  0.91157939,\n",
       "         0.91037671,  0.92238851,  0.92559917,  0.92523048,  0.92927515,\n",
       "         0.93164083,  0.92987674,  0.81003025,  0.80607466,  0.80438477,\n",
       "         0.82893546,  0.82593381,  0.82549046,  0.83844827,  0.83605265,\n",
       "         0.83517546,  0.88319802,  0.88048133,  0.87685302,  0.89843631,\n",
       "         0.89688033,  0.89569916,  0.90768031,  0.90568154,  0.90676237,\n",
       "         0.92317753,  0.92287335,  0.92173815,  0.93729627,  0.93741876,\n",
       "         0.93703136,  0.94420277,  0.94447909,  0.94338443,  0.8114754 ,\n",
       "         0.80384238,  0.80428878,  0.82731355,  0.82629166,  0.82659071,\n",
       "         0.83732004,  0.83596232,  0.83616664,  0.87977941,  0.88106055,\n",
       "         0.88064208,  0.90010986,  0.89782695,  0.89891878,  0.90776047,\n",
       "         0.90805915,  0.90754615,  0.92762164,  0.92740155,  0.92815622,\n",
       "         0.94163724,  0.94383755,  0.94273904,  0.94981438,  0.95136543,\n",
       "         0.94922211,  0.81040901,  0.80464726,  0.80611045,  0.82754093,\n",
       "         0.8257661 ,  0.82464732,  0.840183  ,  0.8366453 ,  0.83483808,\n",
       "         0.88267037,  0.88120462,  0.87831246,  0.90102547,  0.89946989,\n",
       "         0.89941503,  0.90892541,  0.90806978,  0.90756958,  0.93133092,\n",
       "         0.93416437,  0.93122283,  0.9473234 ,  0.94582157,  0.94521894,\n",
       "         0.95246436,  0.9533393 ,  0.95252053]),\n",
       " 'param_max_depth': masked_array(data = [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5\n",
       "  5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       "  7 7 7 7 7 7 7 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
       "  10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_min_weight_fraction_leaf': masked_array(data = [0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
       "  0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.2 0.2 0.2 0.2 0.2 0.2 0.2\n",
       "  0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.05 0.05 0.05 0.05 0.05 0.05\n",
       "  0.05 0.05 0.05 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1\n",
       "  0.1 0.1 0.1 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.2 0.2 0.2 0.2\n",
       "  0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.05 0.05 0.05\n",
       "  0.05 0.05 0.05 0.05 0.05 0.05],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70\n",
       "  100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100\n",
       "  100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120\n",
       "  120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120\n",
       "  70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70\n",
       "  100 100 100 120 120 120],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_subsample': masked_array(data = [0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8}],\n",
       " 'rank_test_score': array([101, 100, 104,  88,  91,  92,  75,  81,  79,  65,  64,  71,  50,\n",
       "         60,  46,  39,  37,  38,   9,   3,  10,  14,  13,  30,   1,  19,\n",
       "         25,  98, 106, 102,  87,  95,  89,  78,  74,  77,  54,  67,  69,\n",
       "         57,  47,  53,  43,  59,  49,   5,   6,  20,  16,  29,  21,  23,\n",
       "          4,  35,  97, 103, 107,  82,  94,  83,  84,  86,  76,  68,  66,\n",
       "         72,  40,  51,  62,  55,  56,  44,  31,  27,  12,  11,  22,  17,\n",
       "         34,   2,  41,  99, 105, 108,  90,  93,  96,  73,  85,  80,  61,\n",
       "         63,  70,  42,  48,  58,  33,  52,  45,  18,  15,  24,  32,   7,\n",
       "          8,  28,  36,  26], dtype=int32),\n",
       " 'split0_test_score': array([ 0.76370456,  0.76488238,  0.76298904,  0.7785335 ,  0.7777769 ,\n",
       "         0.77831693,  0.78925089,  0.7851187 ,  0.78357599,  0.80328591,\n",
       "         0.79975045,  0.79515521,  0.81139359,  0.80078042,  0.81371206,\n",
       "         0.81216197,  0.81261464,  0.81283681,  0.81717349,  0.81383924,\n",
       "         0.82108479,  0.81377542,  0.81549574,  0.81424547,  0.82234578,\n",
       "         0.81372278,  0.81473103,  0.76671175,  0.75463062,  0.76260514,\n",
       "         0.7830242 ,  0.78304609,  0.77737397,  0.78575371,  0.78658687,\n",
       "         0.78580024,  0.81222636,  0.80718307,  0.80513791,  0.80751335,\n",
       "         0.80686285,  0.80470805,  0.80403368,  0.80659324,  0.80892977,\n",
       "         0.82328036,  0.81465271,  0.81132237,  0.81513915,  0.81082304,\n",
       "         0.81411573,  0.81135321,  0.81842973,  0.81326585,  0.76448906,\n",
       "         0.75989565,  0.76028924,  0.78421568,  0.78055098,  0.78473254,\n",
       "         0.78734841,  0.78193166,  0.78530786,  0.80689562,  0.80812312,\n",
       "         0.79607657,  0.80710117,  0.80099981,  0.80182909,  0.80805417,\n",
       "         0.80423877,  0.80732269,  0.81953808,  0.80896396,  0.81887851,\n",
       "         0.81749199,  0.80881095,  0.81576212,  0.81238857,  0.80918869,\n",
       "         0.80572346,  0.76657112,  0.76599063,  0.75680368,  0.77896678,\n",
       "         0.78065364,  0.77242233,  0.78910173,  0.7838321 ,  0.78536408,\n",
       "         0.80324982,  0.80449881,  0.79694991,  0.80445518,  0.8108005 ,\n",
       "         0.80295201,  0.81293385,  0.80456102,  0.80497938,  0.82133935,\n",
       "         0.81360429,  0.81515155,  0.81282532,  0.80831837,  0.81454201,\n",
       "         0.81663008,  0.81038367,  0.81091137]),\n",
       " 'split0_train_score': array([ 0.80969173,  0.80909071,  0.80736946,  0.82750515,  0.82975922,\n",
       "         0.82786029,  0.84611438,  0.83767729,  0.83730993,  0.88177066,\n",
       "         0.87942674,  0.87668966,  0.89774554,  0.89081096,  0.89735544,\n",
       "         0.90741904,  0.90211942,  0.90365131,  0.92114761,  0.9177844 ,\n",
       "         0.91432596,  0.92981393,  0.93079685,  0.92944158,  0.93834541,\n",
       "         0.93741834,  0.93572571,  0.80846604,  0.80703335,  0.80716313,\n",
       "         0.83103729,  0.83175896,  0.82899236,  0.84178852,  0.83846697,\n",
       "         0.83629688,  0.89269121,  0.88739883,  0.88281175,  0.90394724,\n",
       "         0.90402427,  0.90224509,  0.91519589,  0.91081107,  0.91349534,\n",
       "         0.9312012 ,  0.92867017,  0.92799826,  0.94351695,  0.94484765,\n",
       "         0.94261364,  0.95088068,  0.94961949,  0.94818562,  0.8073727 ,\n",
       "         0.80720565,  0.80627685,  0.82995338,  0.83128799,  0.83001779,\n",
       "         0.84057665,  0.8402013 ,  0.83749567,  0.88938762,  0.89000507,\n",
       "         0.88463696,  0.90719043,  0.90436072,  0.90559421,  0.91430964,\n",
       "         0.91390496,  0.91627971,  0.93690846,  0.93518284,  0.93425639,\n",
       "         0.94892093,  0.95107513,  0.94982958,  0.9567624 ,  0.95785914,\n",
       "         0.95432432,  0.81265985,  0.80724367,  0.80781043,  0.82818855,\n",
       "         0.83014161,  0.8299033 ,  0.84360018,  0.84159831,  0.83820784,\n",
       "         0.89165121,  0.88878815,  0.8819918 ,  0.90842609,  0.90725132,\n",
       "         0.90709952,  0.91795666,  0.9149563 ,  0.91524428,  0.94005379,\n",
       "         0.94404375,  0.93744267,  0.95475544,  0.95503597,  0.95187279,\n",
       "         0.95912712,  0.96060777,  0.9579873 ]),\n",
       " 'split1_test_score': array([ 0.76075468,  0.75870965,  0.74974986,  0.76981129,  0.77358372,\n",
       "         0.7708017 ,  0.77937438,  0.77652257,  0.77865068,  0.82401387,\n",
       "         0.82666008,  0.82362149,  0.8272947 ,  0.83090153,  0.8300699 ,\n",
       "         0.83097368,  0.83762385,  0.83362943,  0.84792953,  0.8533739 ,\n",
       "         0.84822609,  0.8509711 ,  0.85267735,  0.83794801,  0.85473467,\n",
       "         0.84625794,  0.84526621,  0.76228429,  0.75880559,  0.75734192,\n",
       "         0.77605948,  0.76763011,  0.77050312,  0.78204435,  0.77987596,\n",
       "         0.78078689,  0.82622962,  0.82385384,  0.82265602,  0.8256277 ,\n",
       "         0.83597341,  0.82753839,  0.83892139,  0.83283166,  0.82763266,\n",
       "         0.84524767,  0.8519039 ,  0.85312132,  0.85059326,  0.85056613,\n",
       "         0.84731752,  0.8557647 ,  0.85370435,  0.84744648,  0.76955997,\n",
       "         0.75533571,  0.75554134,  0.77497219,  0.77122838,  0.77359685,\n",
       "         0.77625563,  0.779038  ,  0.77916842,  0.82260804,  0.82439142,\n",
       "         0.8264985 ,  0.83666468,  0.83103515,  0.83442539,  0.83129834,\n",
       "         0.83400736,  0.83087382,  0.84757935,  0.84727011,  0.84629147,\n",
       "         0.84867272,  0.84853608,  0.8474833 ,  0.83705693,  0.85774159,\n",
       "         0.84272203,  0.76535127,  0.75455362,  0.75347071,  0.77290124,\n",
       "         0.77151991,  0.77037173,  0.7809553 ,  0.77882279,  0.77594289,\n",
       "         0.82993128,  0.82805887,  0.82371768,  0.83471668,  0.82820146,\n",
       "         0.83078   ,  0.8400077 ,  0.83374954,  0.83845766,  0.85212823,\n",
       "         0.84863383,  0.84373666,  0.85849292,  0.85196768,  0.84900695,\n",
       "         0.85034167,  0.85141662,  0.84982058]),\n",
       " 'split1_train_score': array([ 0.80749996,  0.80623077,  0.7990928 ,  0.83084195,  0.82820703,\n",
       "         0.82739516,  0.84573398,  0.8384547 ,  0.83919702,  0.87255093,\n",
       "         0.87674661,  0.87528196,  0.88633314,  0.88899623,  0.8872351 ,\n",
       "         0.90026871,  0.8985121 ,  0.89844849,  0.91550845,  0.91460273,\n",
       "         0.91471949,  0.92192897,  0.92826745,  0.92999541,  0.92557502,\n",
       "         0.93117682,  0.93189648,  0.81142722,  0.80800129,  0.80543972,\n",
       "         0.83206136,  0.82508168,  0.82400981,  0.84109831,  0.83768253,\n",
       "         0.8386026 ,  0.88587582,  0.88166078,  0.87986721,  0.90063804,\n",
       "         0.89866177,  0.89768753,  0.90963346,  0.90988544,  0.90890198,\n",
       "         0.9207633 ,  0.92221529,  0.92082864,  0.9378814 ,  0.93688087,\n",
       "         0.93729527,  0.94376624,  0.94485064,  0.94413431,  0.81370015,\n",
       "         0.80166855,  0.80425447,  0.83034404,  0.83065805,  0.82655885,\n",
       "         0.84110468,  0.84077869,  0.83967787,  0.87904331,  0.8809233 ,\n",
       "         0.88207048,  0.90242512,  0.90017768,  0.90170016,  0.9096341 ,\n",
       "         0.91135332,  0.90810901,  0.92599271,  0.92734151,  0.92879018,\n",
       "         0.94058715,  0.94283842,  0.94316829,  0.94740452,  0.94876977,\n",
       "         0.94977058,  0.81419848,  0.80408458,  0.805312  ,  0.8293679 ,\n",
       "         0.82737235,  0.82602692,  0.84227813,  0.84010238,  0.83393938,\n",
       "         0.88172949,  0.88107348,  0.88181261,  0.90358602,  0.89910177,\n",
       "         0.90068   ,  0.91163935,  0.91040912,  0.90905297,  0.93133207,\n",
       "         0.935261  ,  0.93226804,  0.94661521,  0.94600725,  0.94461452,\n",
       "         0.95176635,  0.95260374,  0.95432785]),\n",
       " 'split2_test_score': array([ 0.8261702 ,  0.83027455,  0.8271512 ,  0.83867958,  0.83308   ,\n",
       "         0.83394449,  0.84245555,  0.83574141,  0.8426418 ,  0.87891698,\n",
       "         0.88089177,  0.87401227,  0.881107  ,  0.88065491,  0.8810014 ,\n",
       "         0.88720243,  0.88606526,  0.88471883,  0.89017749,  0.89218528,\n",
       "         0.88583724,  0.88695004,  0.88465811,  0.88952595,  0.88287813,\n",
       "         0.88847216,  0.88460937,  0.83006941,  0.82426655,  0.82085498,\n",
       "         0.83291882,  0.82641543,  0.83765092,  0.83713674,  0.84511957,\n",
       "         0.84210398,  0.87770228,  0.87215602,  0.87088045,  0.88088184,\n",
       "         0.88100435,  0.88396291,  0.88289176,  0.87316261,  0.88593349,\n",
       "         0.88987671,  0.8916884 ,  0.8826744 ,  0.88397397,  0.88090566,\n",
       "         0.88495037,  0.87782074,  0.88642782,  0.87804542,  0.83452447,\n",
       "         0.82498832,  0.82076441,  0.83725441,  0.82798723,  0.83749135,\n",
       "         0.83174942,  0.8317769 ,  0.84470279,  0.86973207,  0.87217692,\n",
       "         0.87002038,  0.88472355,  0.88683263,  0.87413068,  0.8754211 ,\n",
       "         0.87627803,  0.88735999,  0.87408542,  0.88770523,  0.88772652,\n",
       "         0.88775234,  0.88884794,  0.88629553,  0.88948863,  0.89276267,\n",
       "         0.87951512,  0.82547706,  0.81868644,  0.82494585,  0.83342613,\n",
       "         0.82997844,  0.82970007,  0.84614361,  0.83242986,  0.84265524,\n",
       "         0.87798181,  0.8775871 ,  0.87301012,  0.88872008,  0.8836053 ,\n",
       "         0.87891662,  0.88614065,  0.88011691,  0.88140797,  0.87510277,\n",
       "         0.88809141,  0.88590799,  0.86961004,  0.8958776 ,  0.89232597,\n",
       "         0.87676454,  0.87505613,  0.8833247 ]),\n",
       " 'split2_train_score': array([ 0.80697154,  0.80667447,  0.80877592,  0.82217507,  0.82001827,\n",
       "         0.82066643,  0.83135863,  0.83277679,  0.83243749,  0.86851423,\n",
       "         0.86451773,  0.86251437,  0.88200705,  0.87920702,  0.88115521,\n",
       "         0.89290847,  0.88905185,  0.8914595 ,  0.90578842,  0.90235104,\n",
       "         0.90208466,  0.91542264,  0.9177332 ,  0.91625446,  0.923905  ,\n",
       "         0.92632732,  0.92200803,  0.8101975 ,  0.80318936,  0.80055147,\n",
       "         0.82370774,  0.82096078,  0.82346923,  0.83245797,  0.83200847,\n",
       "         0.8306269 ,  0.87102703,  0.87238439,  0.86788011,  0.89072366,\n",
       "         0.88795496,  0.88716486,  0.89821158,  0.89634812,  0.89788981,\n",
       "         0.91756809,  0.91773458,  0.91638754,  0.93049047,  0.93052777,\n",
       "         0.93118517,  0.93796139,  0.93896714,  0.93783335,  0.81335337,\n",
       "         0.80265294,  0.80233502,  0.82164323,  0.81692895,  0.82319548,\n",
       "         0.83027879,  0.82690696,  0.83132639,  0.8709073 ,  0.87225327,\n",
       "         0.87521881,  0.89071402,  0.88894244,  0.88946195,  0.89933767,\n",
       "         0.89891918,  0.89824972,  0.91996374,  0.9196803 ,  0.9214221 ,\n",
       "         0.93540362,  0.93759909,  0.93521926,  0.94527622,  0.94746738,\n",
       "         0.94357142,  0.80436869,  0.80261353,  0.80520893,  0.82506635,\n",
       "         0.81978433,  0.81801172,  0.83467067,  0.82823522,  0.83236701,\n",
       "         0.87463042,  0.87375224,  0.87113299,  0.89106429,  0.89205656,\n",
       "         0.89046557,  0.89718022,  0.89884393,  0.89841148,  0.92260691,\n",
       "         0.92318837,  0.92395778,  0.94059957,  0.93642148,  0.93916952,\n",
       "         0.94649961,  0.94680638,  0.94524644]),\n",
       " 'std_fit_time': array([ 0.00698256,  0.00414394,  0.00411632,  0.00330572,  0.00993558,\n",
       "         0.00173929,  0.00930058,  0.00572545,  0.01524055,  0.00296804,\n",
       "         0.00844861,  0.01048668,  0.00885599,  0.00490641,  0.0054253 ,\n",
       "         0.00117397,  0.00499667,  0.00637261,  0.01035345,  0.0161479 ,\n",
       "         0.01207043,  0.01095387,  0.00473258,  0.00392334,  0.00945296,\n",
       "         0.01981947,  0.00305264,  0.00333241,  0.02053245,  0.02301291,\n",
       "         0.00378615,  0.01030782,  0.01091149,  0.00867736,  0.01369057,\n",
       "         0.00464771,  0.00360093,  0.00150515,  0.00845375,  0.00768358,\n",
       "         0.01079536,  0.00500533,  0.00392486,  0.00510276,  0.00943038,\n",
       "         0.00824651,  0.00921869,  0.01056373,  0.00688025,  0.00875177,\n",
       "         0.01526424,  0.00785441,  0.00754187,  0.00753328,  0.00629972,\n",
       "         0.00831805,  0.00120612,  0.0020205 ,  0.01082341,  0.00125879,\n",
       "         0.00533007,  0.00786638,  0.00465485,  0.00886708,  0.00746505,\n",
       "         0.00786494,  0.00598443,  0.00934915,  0.00436718,  0.01106173,\n",
       "         0.00866871,  0.00564505,  0.00480013,  0.00911531,  0.01277008,\n",
       "         0.03641841,  0.01895965,  0.00905006,  0.01478073,  0.01667915,\n",
       "         0.00383055,  0.00806473,  0.00172486,  0.00524042,  0.00194975,\n",
       "         0.01056354,  0.00700045,  0.00393991,  0.01407023,  0.00839733,\n",
       "         0.00626901,  0.00728131,  0.01022842,  0.00953275,  0.00429673,\n",
       "         0.00437421,  0.01108048,  0.02470039,  0.0022186 ,  0.00371481,\n",
       "         0.00871158,  0.0007405 ,  0.01936364,  0.0255    ,  0.005597  ,\n",
       "         0.02110242,  0.01940607,  0.02447192]),\n",
       " 'std_score_time': array([  1.59202726e-04,   1.47313273e-04,   7.65152378e-05,\n",
       "          2.84260211e-04,   1.75375228e-04,   3.68885570e-04,\n",
       "          1.12703668e-04,   2.74943316e-04,   1.45212421e-04,\n",
       "          2.50873022e-04,   3.37455332e-04,   2.91320469e-04,\n",
       "          3.15713155e-04,   1.97355322e-04,   1.47360942e-04,\n",
       "          1.66417245e-03,   5.34658057e-06,   2.22754699e-04,\n",
       "          2.16937442e-04,   9.07182606e-05,   1.11767890e-04,\n",
       "          3.43020348e-04,   2.33354268e-04,   1.89562599e-03,\n",
       "          3.46662600e-04,   2.77893284e-04,   2.16423961e-04,\n",
       "          2.07349549e-05,   1.10784767e-04,   3.37897609e-04,\n",
       "          6.26234718e-04,   1.61282489e-04,   2.93188316e-04,\n",
       "          1.99623548e-04,   1.15877698e-04,   2.77745331e-04,\n",
       "          9.16128022e-05,   2.55684409e-04,   3.11350969e-03,\n",
       "          1.03758568e-04,   2.35877188e-04,   2.29497344e-03,\n",
       "          2.11577232e-04,   8.09333241e-04,   2.04058171e-04,\n",
       "          1.94449252e-04,   1.53240820e-04,   7.38508372e-04,\n",
       "          3.38470720e-04,   1.32943296e-03,   2.78967420e-04,\n",
       "          2.03920327e-04,   1.10956351e-04,   1.18544153e-04,\n",
       "          1.67351539e-04,   2.06233617e-04,   1.59208200e-04,\n",
       "          3.16032877e-04,   1.06682835e-04,   2.32139475e-04,\n",
       "          5.24307699e-05,   8.24667161e-05,   2.71086293e-04,\n",
       "          1.21615186e-04,   1.34048218e-04,   1.46833889e-04,\n",
       "          2.97569841e-04,   2.21654319e-03,   2.01838128e-04,\n",
       "          1.17621351e-03,   2.61682128e-04,   4.40565322e-04,\n",
       "          2.62448889e-04,   2.58698771e-04,   2.72815963e-04,\n",
       "          4.70003164e-04,   3.85100402e-04,   2.04317010e-04,\n",
       "          3.62502485e-04,   1.82672444e-04,   3.93931197e-04,\n",
       "          1.01640226e-04,   1.63856116e-04,   5.02499583e-04,\n",
       "          1.38726746e-04,   1.74892049e-04,   2.65621961e-03,\n",
       "          3.45913854e-04,   2.94078727e-04,   3.46225390e-04,\n",
       "          7.05915859e-05,   1.69768352e-04,   1.26960673e-04,\n",
       "          3.89688488e-04,   4.97674099e-04,   3.31410706e-04,\n",
       "          4.65684538e-04,   6.58577352e-05,   9.71580982e-05,\n",
       "          1.01989847e-04,   9.95768610e-05,   5.46373905e-04,\n",
       "          2.10412287e-04,   1.94189880e-04,   7.37381382e-05,\n",
       "          1.82131925e-03,   1.86874578e-04,   2.75216636e-04]),\n",
       " 'std_test_score': array([ 0.03015119,  0.03236362,  0.03378603,  0.03060214,  0.02709938,\n",
       "         0.02814858,  0.02769081,  0.02611431,  0.02906029,  0.03189625,\n",
       "         0.03373294,  0.03259345,  0.02981696,  0.03292366,  0.02864145,\n",
       "         0.03186551,  0.03047888,  0.0301896 ,  0.02991708,  0.03197738,\n",
       "         0.02654153,  0.0298677 ,  0.02825615,  0.03141511,  0.02472728,\n",
       "         0.03059211,  0.02859445,  0.03094837,  0.03187268,  0.02876616,\n",
       "         0.02531019,  0.02487636,  0.03015047,  0.0251299 ,  0.0292886 ,\n",
       "         0.02778546,  0.02813895,  0.02754111,  0.02778618,  0.03119198,\n",
       "         0.03048944,  0.03329669,  0.03225512,  0.02736975,  0.03277854,\n",
       "         0.02769683,  0.03144714,  0.02926739,  0.02809947,  0.02869175,\n",
       "         0.02892901,  0.02764022,  0.02776032,  0.02645385,  0.03187155,\n",
       "         0.03179879,  0.02967626,  0.02742916,  0.02484056,  0.02785581,\n",
       "         0.02396606,  0.02419634,  0.02953831,  0.02668834,  0.02717282,\n",
       "         0.03033504,  0.03197643,  0.03555004,  0.0295559 ,  0.02792871,\n",
       "         0.02954757,  0.0335708 ,  0.02226664,  0.03214159,  0.02829119,\n",
       "         0.02873546,  0.03266712,  0.02883516,  0.03213634,  0.03426212,\n",
       "         0.030118  ,  0.02804672,  0.02791689,  0.03292014,  0.02720181,\n",
       "         0.02566491,  0.02748361,  0.02898741,  0.02416487,  0.02946592,\n",
       "         0.03091077,  0.03044752,  0.03149032,  0.03484038,  0.03102921,\n",
       "         0.03136832,  0.03021143,  0.03109941,  0.03127213,  0.0220222 ,\n",
       "         0.03041881,  0.0290535 ,  0.02457285,  0.03573713,  0.03181416,\n",
       "         0.0246053 ,  0.026716  ,  0.0295839 ]),\n",
       " 'std_train_score': array([ 0.00117769,  0.00125673,  0.00427192,  0.0035693 ,  0.00427331,\n",
       "         0.00328707,  0.00686802,  0.00251347,  0.00284786,  0.00554809,\n",
       "         0.00648937,  0.00637645,  0.00663876,  0.00509655,  0.00668193,\n",
       "         0.00592412,  0.0055103 ,  0.00499506,  0.00634371,  0.00665345,\n",
       "         0.00586556,  0.0058842 ,  0.00565712,  0.00635103,  0.00644978,\n",
       "         0.00453977,  0.00577946,  0.00121467,  0.00207814,  0.00280038,\n",
       "         0.00372012,  0.00444932,  0.00248603,  0.00424514,  0.00287755,\n",
       "         0.00335123,  0.00904478,  0.00618609,  0.00645768,  0.00561851,\n",
       "         0.00668011,  0.00631498,  0.00707002,  0.00661054,  0.00654811,\n",
       "         0.00582162,  0.00448862,  0.00478349,  0.00533411,  0.00585843,\n",
       "         0.00466938,  0.0052833 ,  0.00435674,  0.00425943,  0.0029045 ,\n",
       "         0.00241191,  0.00160943,  0.00401269,  0.00662543,  0.00278529,\n",
       "         0.00498358,  0.00640744,  0.00353662,  0.00756249,  0.00724779,\n",
       "         0.00397539,  0.00692283,  0.00651026,  0.00687336,  0.00625422,\n",
       "         0.00654635,  0.00737146,  0.00701289,  0.00632903,  0.00525872,\n",
       "         0.00556815,  0.00554675,  0.00597235,  0.00498923,  0.00462243,\n",
       "         0.00440695,  0.00431709,  0.00193167,  0.0012028 ,  0.00181482,\n",
       "         0.00437823,  0.00495176,  0.00393499,  0.0059781 ,  0.00246773,\n",
       "         0.00698048,  0.00613909,  0.00507718,  0.00731552,  0.00620869,\n",
       "         0.00684944,  0.00869633,  0.00678265,  0.00695155,  0.00712266,\n",
       "         0.00854941,  0.00555457,  0.00580076,  0.00760047,  0.00520367,\n",
       "         0.00517873,  0.00565835,  0.00535613])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.1,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=0.7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17198.96120694  23195.83272626  21463.95294502  20761.49425872\n",
      "  20182.72379423  22025.80395473  18995.875571    18174.82013286\n",
      "  15579.48662801  16773.60645665]\n",
      "[ 0.86531833  0.77064438  0.79723221  0.90047097  0.83797817  0.7653455\n",
      "  0.85238884  0.84791002  0.90572437  0.86144469]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor(n_estimators= 100, max_depth=5, subsample= 0.7\n",
    "                                      ,min_weight_fraction_leaf=0.1)\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'), cv=10)\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2'])                         \n",
    "\n",
    "score_dataset_cv(reduced_X_train, reduced_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
