{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Cross validation: Gradient Bossting vs RandomForest\n",
    "\n",
    "Realicemos una predicción basada en un Gradient Bossting. \n",
    "Se parte de los datos analizados, normalizados y acotados logrados en el punto 0, para el training.\n",
    "\n",
    "En este caso no vamos a jugar tanto con los parámetros, como con los modelos que existen. Utilizaremos uno de tipo arboles de decisión (randomforest) y uno de regresiones lineales (Gradient Bossting) y veremos cual se comporta mejor de base, mediante un Cross validation\n",
    "\n",
    "\n",
    "## Importación de datos y selección de variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>728.805765</td>\n",
       "      <td>729.805765</td>\n",
       "      <td>56.877145</td>\n",
       "      <td>10460.434454</td>\n",
       "      <td>6.094715</td>\n",
       "      <td>5.576527</td>\n",
       "      <td>1971.194235</td>\n",
       "      <td>1984.818806</td>\n",
       "      <td>439.128346</td>\n",
       "      <td>46.645161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.082361</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.868909</td>\n",
       "      <td>0.069321</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.821551</td>\n",
       "      <td>0.084420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.402158</td>\n",
       "      <td>421.402158</td>\n",
       "      <td>42.339638</td>\n",
       "      <td>9862.564977</td>\n",
       "      <td>1.376542</td>\n",
       "      <td>1.113638</td>\n",
       "      <td>30.190353</td>\n",
       "      <td>20.640669</td>\n",
       "      <td>432.964939</td>\n",
       "      <td>161.471529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.275008</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.337616</td>\n",
       "      <td>0.254086</td>\n",
       "      <td>0.052342</td>\n",
       "      <td>0.090410</td>\n",
       "      <td>0.116395</td>\n",
       "      <td>0.383022</td>\n",
       "      <td>0.278112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>364.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7540.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>729.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9473.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1093.000000</td>\n",
       "      <td>1094.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>11600.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2188.000000</td>\n",
       "      <td>1474.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           Id   MSSubClass        LotArea  OverallQual  \\\n",
       "count  1457.000000  1457.000000  1457.000000    1457.000000  1457.000000   \n",
       "mean    728.805765   729.805765    56.877145   10460.434454     6.094715   \n",
       "std     421.402158   421.402158    42.339638    9862.564977     1.376542   \n",
       "min       0.000000     1.000000    20.000000    1300.000000     1.000000   \n",
       "25%     364.000000   365.000000    20.000000    7540.000000     5.000000   \n",
       "50%     729.000000   730.000000    50.000000    9473.000000     6.000000   \n",
       "75%    1093.000000  1094.000000    70.000000   11600.000000     7.000000   \n",
       "max    1459.000000  1460.000000   190.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   BsmtFinSF1   BsmtFinSF2  \\\n",
       "count  1457.000000  1457.000000   1457.000000  1457.000000  1457.000000   \n",
       "mean      5.576527  1971.194235   1984.818806   439.128346    46.645161   \n",
       "std       1.113638    30.190353     20.640669   432.964939   161.471529   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1972.000000   1994.000000   383.000000     0.000000   \n",
       "75%       6.000000  2000.000000   2004.000000   712.000000     0.000000   \n",
       "max       9.000000  2010.000000   2010.000000  2188.000000  1474.000000   \n",
       "\n",
       "               ...            SaleType_ConLw  SaleType_New  SaleType_Oth  \\\n",
       "count          ...               1457.000000   1457.000000   1457.000000   \n",
       "mean           ...                  0.003432      0.082361      0.002059   \n",
       "std            ...                  0.058500      0.275008      0.045345   \n",
       "min            ...                  0.000000      0.000000      0.000000   \n",
       "25%            ...                  0.000000      0.000000      0.000000   \n",
       "50%            ...                  0.000000      0.000000      0.000000   \n",
       "75%            ...                  0.000000      0.000000      0.000000   \n",
       "max            ...                  1.000000      1.000000      1.000000   \n",
       "\n",
       "       SaleType_WD  SaleCondition_Abnorml  SaleCondition_AdjLand  \\\n",
       "count  1457.000000            1457.000000            1457.000000   \n",
       "mean      0.868909               0.069321               0.002745   \n",
       "std       0.337616               0.254086               0.052342   \n",
       "min       0.000000               0.000000               0.000000   \n",
       "25%       1.000000               0.000000               0.000000   \n",
       "50%       1.000000               0.000000               0.000000   \n",
       "75%       1.000000               0.000000               0.000000   \n",
       "max       1.000000               1.000000               1.000000   \n",
       "\n",
       "       SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "count           1457.000000           1457.000000           1457.000000   \n",
       "mean               0.008236              0.013727              0.821551   \n",
       "std                0.090410              0.116395              0.383022   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%                0.000000              0.000000              1.000000   \n",
       "50%                0.000000              0.000000              1.000000   \n",
       "75%                0.000000              0.000000              1.000000   \n",
       "max                1.000000              1.000000              1.000000   \n",
       "\n",
       "       SaleCondition_Partial  \n",
       "count            1457.000000  \n",
       "mean                0.084420  \n",
       "std                 0.278112  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 0.000000  \n",
       "75%                 0.000000  \n",
       "max                 1.000000  \n",
       "\n",
       "[8 rows x 222 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Librerías a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importación de datos\n",
    "melbourne_data = pd.read_csv(\"data/PreciosCasas/train_final.csv\", sep='\\t', encoding='utf-8') \n",
    "\n",
    "# print a summary of the data in Melbourne data\n",
    "melbourne_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Id', 'MSSubClass', 'LotArea', 'OverallQual',\n",
      "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       ...\n",
      "       'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD',\n",
      "       'SaleCondition_Abnorml', 'SaleCondition_AdjLand',\n",
      "       'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal',\n",
      "       'SaleCondition_Partial'],\n",
      "      dtype='object', length=222)\n"
     ]
    }
   ],
   "source": [
    "#Vamos a ver que variables elegimos\n",
    "\n",
    "print(melbourne_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos prededir el precio, será nuestro target, para lo cual, cogeremos unas variables como pedictores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    12.247694\n",
      "1    12.109011\n",
      "2    12.317167\n",
      "3    11.849398\n",
      "4    12.429216\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10460.434454</td>\n",
       "      <td>1971.194235</td>\n",
       "      <td>1159.129032</td>\n",
       "      <td>345.560055</td>\n",
       "      <td>1.563487</td>\n",
       "      <td>2.866163</td>\n",
       "      <td>6.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9862.564977</td>\n",
       "      <td>30.190353</td>\n",
       "      <td>372.015864</td>\n",
       "      <td>435.505117</td>\n",
       "      <td>0.549961</td>\n",
       "      <td>0.816595</td>\n",
       "      <td>1.616384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7540.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9473.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11600.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1391.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215245.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>3228.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LotArea    YearBuilt     1stFlrSF     2ndFlrSF     FullBath  \\\n",
       "count    1457.000000  1457.000000  1457.000000  1457.000000  1457.000000   \n",
       "mean    10460.434454  1971.194235  1159.129032   345.560055     1.563487   \n",
       "std      9862.564977    30.190353   372.015864   435.505117     0.549961   \n",
       "min      1300.000000  1872.000000   334.000000     0.000000     0.000000   \n",
       "25%      7540.000000  1954.000000   882.000000     0.000000     1.000000   \n",
       "50%      9473.000000  1972.000000  1086.000000     0.000000     2.000000   \n",
       "75%     11600.000000  2000.000000  1391.000000   728.000000     2.000000   \n",
       "max    215245.000000  2010.000000  3228.000000  2065.000000     3.000000   \n",
       "\n",
       "       BedroomAbvGr  TotRmsAbvGrd  \n",
       "count   1457.000000   1457.000000  \n",
       "mean       2.866163      6.510638  \n",
       "std        0.816595      1.616384  \n",
       "min        0.000000      2.000000  \n",
       "25%        2.000000      5.000000  \n",
       "50%        3.000000      6.000000  \n",
       "75%        3.000000      7.000000  \n",
       "max        8.000000     14.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= melbourne_data.SalePrice\n",
    "print(y.head())\n",
    "melbourne_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = melbourne_data[melbourne_predictors]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Random forest \n",
    "\n",
    "Pasamos directamente a modelar sin más (el objeto no son los parámetros, es comparar los modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "\n",
    "from  sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Separamos los datos en dos grupos, \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.133609440716\n"
     ]
    }
   ],
   "source": [
    "# 1) RandomForest\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(X_train, y_train)\n",
    "melb_preds = forest_model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, melb_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14156713  0.14520264  0.13336128]\n",
      "[ 0.73456125  0.76459303  0.77818127]\n"
     ]
    }
   ],
   "source": [
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'))\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2']) \n",
    "    \n",
    "score_dataset_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo GradientBoostingRegressor \n",
    "\n",
    "Pasamos directamente a modelar sin más (el objeto no son los parámetros, es comparar los modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13375915  0.13823556  0.12399015]\n",
      "[ 0.75139868  0.7800183   0.81837829]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor()\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'))\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2'])                         \n",
    "\n",
    "score_dataset_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras en el modelo - Hiperparámetros\n",
    "\n",
    "Tenemos los dos modelos y se comportan de forma bastante similar (parecía que uno lineal podía ser mejor, pero no termina de ser tan obvio), así que vamos a hacer mirar que parámetros son los que debo cambiar para mi modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [70, 100, 120], 'max_depth': [3, 5, 7, 10], 'subsample': [0.6, 0.7, 0.8], 'min_weight_fraction_leaf': [0.2, 0.1, 0.05]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators': [70, 100, 120], 'max_depth': [3, 5, 7, 10], 'subsample': [0.60, 0.7, 0.80],'min_weight_fraction_leaf':[0.20, 0.1, 0.05]}\n",
    "gbr = GradientBoostingRegressor()\n",
    "clf = GridSearchCV(gbr, parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.0609986 ,  0.05113602,  0.05222217,  0.09214973,  0.08438269,\n",
       "         0.08170168,  0.09218542,  0.10509396,  0.09426618,  0.05707097,\n",
       "         0.06233708,  0.0645109 ,  0.08340208,  0.07685804,  0.08996288,\n",
       "         0.10349623,  0.10861866,  0.09865777,  0.06299774,  0.0649151 ,\n",
       "         0.11139361,  0.17240024,  0.17814382,  0.16331951,  0.1204435 ,\n",
       "         0.10525878,  0.10543172,  0.05417323,  0.04990101,  0.05515361,\n",
       "         0.09470089,  0.08980044,  0.08696946,  0.10188882,  0.09827542,\n",
       "         0.09439476,  0.06897259,  0.0933431 ,  0.09260909,  0.10011546,\n",
       "         0.10621397,  0.10553567,  0.11838031,  0.12657666,  0.13329109,\n",
       "         0.08023659,  0.08063094,  0.0798924 ,  0.11103646,  0.11555052,\n",
       "         0.10778944,  0.13073039,  0.13452752,  0.13723675,  0.05878297,\n",
       "         0.05907623,  0.06086405,  0.15417035,  0.15760676,  0.15160426,\n",
       "         0.09794474,  0.12026413,  0.09958625,  0.07593926,  0.06751283,\n",
       "         0.06990274,  0.10141937,  0.10395789,  0.09996454,  0.11274322,\n",
       "         0.12147522,  0.12463888,  0.08402904,  0.08398223,  0.09046896,\n",
       "         0.12014167,  0.13461264,  0.12374258,  0.15254807,  0.15665897,\n",
       "         0.15878638,  0.05692808,  0.06115262,  0.0646441 ,  0.07961432,\n",
       "         0.0837903 ,  0.08405892,  0.15877748,  0.17964141,  0.17863226,\n",
       "         0.07091244,  0.07602795,  0.10783291,  0.12080733,  0.12313795,\n",
       "         0.13032969,  0.15016341,  0.18845479,  0.13901806,  0.11972284,\n",
       "         0.20202835,  0.140378  ,  0.16581273,  0.17999967,  0.1545879 ,\n",
       "         0.18680882,  0.18948857,  0.176699  ]),\n",
       " 'mean_score_time': array([ 0.00151141,  0.00158676,  0.00159375,  0.00225631,  0.00167068,\n",
       "         0.00200303,  0.00205056,  0.00200891,  0.00205199,  0.00142169,\n",
       "         0.00155449,  0.00287596,  0.00188152,  0.00168133,  0.00190314,\n",
       "         0.00218852,  0.00215197,  0.00189821,  0.00176843,  0.00165049,\n",
       "         0.00377774,  0.00249736,  0.00222985,  0.00239682,  0.00204158,\n",
       "         0.0020504 ,  0.00206765,  0.00130574,  0.00122865,  0.00186753,\n",
       "         0.00172718,  0.00193501,  0.00189193,  0.00184449,  0.00190131,\n",
       "         0.00178305,  0.00178337,  0.00369477,  0.00335725,  0.00285618,\n",
       "         0.00226561,  0.00189654,  0.00263079,  0.00275373,  0.00230344,\n",
       "         0.00181127,  0.00170477,  0.00184274,  0.00201567,  0.00223986,\n",
       "         0.00236924,  0.00251985,  0.00258748,  0.00232561,  0.00131838,\n",
       "         0.0015707 ,  0.00144847,  0.00205723,  0.00304023,  0.00168006,\n",
       "         0.00193771,  0.00225274,  0.00154034,  0.00190687,  0.00172043,\n",
       "         0.00190401,  0.00209403,  0.00305327,  0.00221674,  0.00236464,\n",
       "         0.00326991,  0.00239627,  0.00188327,  0.00201734,  0.00211628,\n",
       "         0.00237775,  0.00270351,  0.00370351,  0.00352105,  0.00287255,\n",
       "         0.00290322,  0.00146317,  0.00149854,  0.00133944,  0.00140627,\n",
       "         0.00183193,  0.00175214,  0.00170628,  0.0024399 ,  0.00218391,\n",
       "         0.00199246,  0.00178067,  0.00270184,  0.00242575,  0.00254448,\n",
       "         0.0023466 ,  0.00409245,  0.00355951,  0.00286921,  0.00278823,\n",
       "         0.00633987,  0.00275302,  0.00719349,  0.00357699,  0.00306654,\n",
       "         0.00516677,  0.00361164,  0.00303157]),\n",
       " 'mean_test_score': array([ 0.709005  ,  0.71123011,  0.70757378,  0.71845701,  0.71843809,\n",
       "         0.71715308,  0.7197359 ,  0.71784435,  0.71610208,  0.75982521,\n",
       "         0.76310632,  0.76058105,  0.76789445,  0.76812216,  0.76476024,\n",
       "         0.77106673,  0.76936635,  0.77102761,  0.77773105,  0.77731783,\n",
       "         0.7776423 ,  0.78153283,  0.78375673,  0.77897606,  0.78754047,\n",
       "         0.78511382,  0.78181127,  0.70548398,  0.712587  ,  0.70731078,\n",
       "         0.71757147,  0.71498522,  0.715428  ,  0.7183698 ,  0.71870602,\n",
       "         0.71579037,  0.76709135,  0.76356406,  0.76163646,  0.76555965,\n",
       "         0.76634424,  0.76549431,  0.7705414 ,  0.77013114,  0.76744255,\n",
       "         0.78308619,  0.78463927,  0.77815552,  0.78238951,  0.78448724,\n",
       "         0.78251698,  0.78874752,  0.78122614,  0.78397563,  0.70661765,\n",
       "         0.7081826 ,  0.70819571,  0.71517098,  0.71549483,  0.71810756,\n",
       "         0.71910929,  0.72047117,  0.72062805,  0.76466022,  0.76638244,\n",
       "         0.76166578,  0.77129711,  0.76932888,  0.76527365,  0.77012375,\n",
       "         0.77016533,  0.77004479,  0.78253025,  0.78326614,  0.78031391,\n",
       "         0.78410483,  0.7844575 ,  0.78133079,  0.78056233,  0.78023224,\n",
       "         0.78326185,  0.70924   ,  0.70684265,  0.70817509,  0.71474275,\n",
       "         0.71798213,  0.71705228,  0.71903564,  0.71738062,  0.71914888,\n",
       "         0.76558176,  0.76097813,  0.76246383,  0.77040557,  0.76869396,\n",
       "         0.7704567 ,  0.77284897,  0.76933307,  0.76678337,  0.78354283,\n",
       "         0.78166243,  0.78481545,  0.7842926 ,  0.78232767,  0.7814848 ,\n",
       "         0.78094341,  0.78548452,  0.78278924]),\n",
       " 'mean_train_score': array([ 0.74338271,  0.74688076,  0.74393176,  0.75816653,  0.75876233,\n",
       "         0.75764411,  0.76588871,  0.76373663,  0.76310537,  0.8114005 ,\n",
       "         0.81131275,  0.80869143,  0.82562208,  0.82309524,  0.82244506,\n",
       "         0.83074798,  0.83285427,  0.83029972,  0.84122601,  0.84325974,\n",
       "         0.84210026,  0.85528739,  0.85616528,  0.8531212 ,  0.86471111,\n",
       "         0.86254264,  0.86039459,  0.74410948,  0.74571773,  0.7439006 ,\n",
       "         0.75816949,  0.75747622,  0.75659627,  0.76445238,  0.76458019,\n",
       "         0.76337449,  0.82365707,  0.82313043,  0.82240713,  0.83956966,\n",
       "         0.8405853 ,  0.83718955,  0.84752418,  0.84739212,  0.84507244,\n",
       "         0.86740941,  0.86745669,  0.86547909,  0.88295905,  0.88204685,\n",
       "         0.88063877,  0.89124872,  0.88980776,  0.88825741,  0.74603265,\n",
       "         0.74314913,  0.74328671,  0.75650373,  0.75719468,  0.75914824,\n",
       "         0.76391956,  0.76551256,  0.76467822,  0.82629945,  0.82603377,\n",
       "         0.82335617,  0.84262438,  0.84202763,  0.83985756,  0.84932034,\n",
       "         0.85061168,  0.84857736,  0.87944634,  0.87926438,  0.87616985,\n",
       "         0.89379554,  0.89555754,  0.89423203,  0.903718  ,  0.90506438,\n",
       "         0.90242657,  0.74420858,  0.74414214,  0.74330308,  0.75874255,\n",
       "         0.75860126,  0.75808635,  0.76349454,  0.76627842,  0.76449341,\n",
       "         0.82593229,  0.82546702,  0.82335896,  0.84167674,  0.84266417,\n",
       "         0.83897428,  0.84904042,  0.85087653,  0.84813866,  0.88230449,\n",
       "         0.88161521,  0.88137946,  0.90102533,  0.899809  ,  0.89867937,\n",
       "         0.91044834,  0.90899446,  0.90864888]),\n",
       " 'param_max_depth': masked_array(data = [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5\n",
       "  5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
       "  7 7 7 7 7 7 7 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
       "  10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_min_weight_fraction_leaf': masked_array(data = [0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
       "  0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.2 0.2 0.2 0.2 0.2 0.2 0.2\n",
       "  0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.05 0.05 0.05 0.05 0.05 0.05\n",
       "  0.05 0.05 0.05 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1\n",
       "  0.1 0.1 0.1 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.2 0.2 0.2 0.2\n",
       "  0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.05 0.05 0.05\n",
       "  0.05 0.05 0.05 0.05 0.05 0.05],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70\n",
       "  100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100\n",
       "  100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120\n",
       "  120 120 70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120\n",
       "  70 70 70 100 100 100 120 120 120 70 70 70 100 100 100 120 120 120 70 70 70\n",
       "  100 100 100 120 120 120],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_subsample': masked_array(data = [0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8\n",
       "  0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8 0.6 0.7 0.8],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 3,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 5,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 7,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.2,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.1,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.8},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.6},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.7},\n",
       "  {'max_depth': 10,\n",
       "   'min_weight_fraction_leaf': 0.05,\n",
       "   'n_estimators': 120,\n",
       "   'subsample': 0.8}],\n",
       " 'rank_test_score': array([100,  98, 104,  80,  81,  88,  75,  85,  90,  72,  66,  71,  53,\n",
       "         52,  63,  39,  48,  40,  34,  36,  35,  24,  12,  32,   2,   4,\n",
       "         22, 108,  97, 105,  86,  95,  93,  82,  79,  91,  55,  65,  69,\n",
       "         60,  58,  61,  41,  45,  54,  16,   6,  33,  20,   7,  19,   1,\n",
       "         27,  11, 107, 102, 101,  94,  92,  83,  77,  74,  73,  64,  57,\n",
       "         68,  38,  50,  62,  46,  44,  47,  18,  14,  30,  10,   8,  26,\n",
       "         29,  31,  15,  99, 106, 103,  96,  84,  89,  78,  87,  76,  59,\n",
       "         70,  67,  43,  51,  42,  37,  49,  56,  13,  23,   5,   9,  21,\n",
       "         25,  28,   3,  17], dtype=int32),\n",
       " 'split0_test_score': array([ 0.67829662,  0.68340698,  0.67614063,  0.68351072,  0.67978848,\n",
       "         0.68268422,  0.67995135,  0.67717067,  0.67747526,  0.72797393,\n",
       "         0.73288227,  0.72941179,  0.74117496,  0.73759058,  0.73307094,\n",
       "         0.74649354,  0.74057997,  0.7394603 ,  0.73751446,  0.74682268,\n",
       "         0.74914102,  0.74958746,  0.7532206 ,  0.74788273,  0.75240944,\n",
       "         0.75397966,  0.74876033,  0.6686827 ,  0.68210341,  0.67536126,\n",
       "         0.68047877,  0.68213599,  0.67210331,  0.68412954,  0.6803734 ,\n",
       "         0.67896263,  0.74003218,  0.73420122,  0.73097819,  0.733621  ,\n",
       "         0.73796728,  0.73799392,  0.73905522,  0.73982272,  0.73679853,\n",
       "         0.75231161,  0.75206812,  0.74817023,  0.75027974,  0.75241672,\n",
       "         0.752508  ,  0.76168686,  0.75377519,  0.75294599,  0.67307315,\n",
       "         0.67529484,  0.67645982,  0.68209139,  0.67852329,  0.67885748,\n",
       "         0.68356792,  0.67888149,  0.68124677,  0.73720609,  0.74022723,\n",
       "         0.7296408 ,  0.74148419,  0.73854057,  0.72886174,  0.74474391,\n",
       "         0.73765466,  0.74620228,  0.75660988,  0.74784387,  0.75177608,\n",
       "         0.7552546 ,  0.75957743,  0.75191778,  0.73860479,  0.74924999,\n",
       "         0.75022609,  0.67438176,  0.67523322,  0.67230884,  0.67809544,\n",
       "         0.67898579,  0.68007901,  0.67721464,  0.68229786,  0.68023011,\n",
       "         0.74238006,  0.73230174,  0.73590563,  0.74084289,  0.7371778 ,\n",
       "         0.74255079,  0.74489097,  0.73813509,  0.73828437,  0.74977481,\n",
       "         0.74955959,  0.75050751,  0.75120921,  0.74863551,  0.74475001,\n",
       "         0.74826432,  0.75264192,  0.74831397]),\n",
       " 'split0_train_score': array([ 0.76108153,  0.76451927,  0.75948547,  0.77410956,  0.77624447,\n",
       "         0.77343905,  0.7811241 ,  0.78092224,  0.78071744,  0.82203807,\n",
       "         0.82245024,  0.82140565,  0.83603117,  0.83256884,  0.83469054,\n",
       "         0.84002623,  0.84337457,  0.84190178,  0.85220503,  0.85711956,\n",
       "         0.85300157,  0.86720662,  0.86544274,  0.86428872,  0.87576384,\n",
       "         0.8724076 ,  0.87188028,  0.7584141 ,  0.76483851,  0.760689  ,\n",
       "         0.77402709,  0.77891822,  0.77323879,  0.78342411,  0.78230201,\n",
       "         0.78088633,  0.83475306,  0.83425364,  0.83480522,  0.8491923 ,\n",
       "         0.85172332,  0.84719395,  0.85954077,  0.85755563,  0.85569716,\n",
       "         0.87751803,  0.87835083,  0.87479559,  0.89802509,  0.89186989,\n",
       "         0.89113885,  0.90167538,  0.90164826,  0.89752337,  0.76359494,\n",
       "         0.75777445,  0.76094931,  0.77601644,  0.7762983 ,  0.77495882,\n",
       "         0.78388636,  0.78484372,  0.78225963,  0.83846265,  0.83789445,\n",
       "         0.83626616,  0.85210806,  0.85366577,  0.85170137,  0.860013  ,\n",
       "         0.86025535,  0.85746265,  0.88948172,  0.88936217,  0.88493108,\n",
       "         0.90392738,  0.90718206,  0.90468931,  0.91368866,  0.91417176,\n",
       "         0.91286815,  0.76234273,  0.75986964,  0.75507678,  0.77484842,\n",
       "         0.77310404,  0.77561018,  0.7803738 ,  0.78639744,  0.78093991,\n",
       "         0.83670514,  0.8345079 ,  0.83685836,  0.85084401,  0.85294607,\n",
       "         0.85102113,  0.8562856 ,  0.85809415,  0.85733985,  0.89409918,\n",
       "         0.8928586 ,  0.89262475,  0.90697566,  0.90839182,  0.9101855 ,\n",
       "         0.91917876,  0.91785255,  0.91762554]),\n",
       " 'split1_test_score': array([ 0.70905135,  0.70972971,  0.70922759,  0.72146297,  0.72085894,\n",
       "         0.71932013,  0.72646868,  0.72583626,  0.72272234,  0.7680829 ,\n",
       "         0.76808265,  0.7693418 ,  0.77122928,  0.77476195,  0.77027955,\n",
       "         0.77509751,  0.7761691 ,  0.7742572 ,  0.78207013,  0.7838611 ,\n",
       "         0.78411776,  0.78233852,  0.78921292,  0.78174656,  0.78876431,\n",
       "         0.7872874 ,  0.78349879,  0.70927249,  0.71374247,  0.70947308,\n",
       "         0.72088702,  0.71970089,  0.72237337,  0.72003745,  0.72320022,\n",
       "         0.71911547,  0.76875446,  0.7699364 ,  0.76741778,  0.77393258,\n",
       "         0.77400516,  0.77196107,  0.77587669,  0.77781877,  0.77252368,\n",
       "         0.79066112,  0.79148286,  0.78323911,  0.78710633,  0.79102179,\n",
       "         0.78827941,  0.78795696,  0.7850612 ,  0.78444435,  0.70735899,\n",
       "         0.70871188,  0.70733894,  0.71753788,  0.71995289,  0.72510522,\n",
       "         0.72319134,  0.72497156,  0.725741  ,  0.77106116,  0.77084389,\n",
       "         0.77026186,  0.77283555,  0.77944648,  0.77707157,  0.77825646,\n",
       "         0.77977836,  0.77071613,  0.78616411,  0.7900435 ,  0.78789927,\n",
       "         0.78927593,  0.78632352,  0.78687677,  0.78796054,  0.78580687,\n",
       "         0.78977585,  0.70772787,  0.7064314 ,  0.71180604,  0.71722055,\n",
       "         0.72086681,  0.72238069,  0.72785814,  0.7213248 ,  0.72289985,\n",
       "         0.77006459,  0.76618837,  0.76600552,  0.77593643,  0.77523711,\n",
       "         0.77666418,  0.77975761,  0.77763589,  0.77386518,  0.79226123,\n",
       "         0.78602399,  0.79096614,  0.78903515,  0.79219925,  0.78850845,\n",
       "         0.78855777,  0.78734475,  0.79223916]),\n",
       " 'split1_train_score': array([ 0.73769357,  0.74242685,  0.73903281,  0.75412708,  0.75207539,\n",
       "         0.75270374,  0.76455249,  0.75863038,  0.75838171,  0.81307378,\n",
       "         0.81151653,  0.80692897,  0.82769896,  0.82615746,  0.8217492 ,\n",
       "         0.83207969,  0.83248475,  0.83104303,  0.84123736,  0.84519554,\n",
       "         0.84393317,  0.85679606,  0.85883154,  0.85598425,  0.86470046,\n",
       "         0.86324985,  0.8613104 ,  0.73904542,  0.73929176,  0.73967468,\n",
       "         0.75169164,  0.75000442,  0.75204938,  0.75888227,  0.75978869,\n",
       "         0.75721013,  0.82233434,  0.82170114,  0.81974555,  0.83987732,\n",
       "         0.83963853,  0.83707152,  0.84697883,  0.84990216,  0.84543668,\n",
       "         0.86542981,  0.86651162,  0.86724148,  0.88138394,  0.8840864 ,\n",
       "         0.87996859,  0.8908793 ,  0.88899001,  0.88952825,  0.74061113,\n",
       "         0.73584919,  0.73722898,  0.75108777,  0.75424102,  0.75618757,\n",
       "         0.75726652,  0.75606268,  0.75995238,  0.82521533,  0.82503135,\n",
       "         0.82338635,  0.84342362,  0.84200612,  0.84061678,  0.85112281,\n",
       "         0.85254283,  0.84988872,  0.87662884,  0.87879126,  0.8764619 ,\n",
       "         0.89495239,  0.89364051,  0.89556892,  0.90436986,  0.90531423,\n",
       "         0.9028321 ,  0.73564159,  0.73711426,  0.73878064,  0.75553412,\n",
       "         0.75511878,  0.75342838,  0.75821371,  0.75908241,  0.75786072,\n",
       "         0.82775141,  0.82678335,  0.82077294,  0.84366876,  0.84443457,\n",
       "         0.83801283,  0.85084273,  0.85231095,  0.84957724,  0.87906132,\n",
       "         0.88066135,  0.87990959,  0.89960957,  0.9004519 ,  0.89749485,\n",
       "         0.91081432,  0.90686935,  0.9092942 ]),\n",
       " 'split2_test_score': array([ 0.73966703,  0.74055363,  0.73735311,  0.75039734,  0.75466684,\n",
       "         0.7494549 ,  0.75278768,  0.75052611,  0.74810865,  0.78341881,\n",
       "         0.78835405,  0.78298955,  0.79127911,  0.79201395,  0.79093024,\n",
       "         0.79160916,  0.79134999,  0.79936535,  0.81360855,  0.8012697 ,\n",
       "         0.79966811,  0.81267252,  0.80883668,  0.80729888,  0.82144766,\n",
       "         0.8140744 ,  0.81317467,  0.73849673,  0.74191512,  0.73709801,\n",
       "         0.75134862,  0.74311877,  0.75180733,  0.75094241,  0.75254442,\n",
       "         0.74929301,  0.79248741,  0.78655457,  0.78651341,  0.78912538,\n",
       "         0.78706029,  0.78652793,  0.7966923 ,  0.79275192,  0.79300543,\n",
       "         0.80628584,  0.81036682,  0.80305722,  0.80978246,  0.8100232 ,\n",
       "         0.80676353,  0.81659874,  0.80484203,  0.81453655,  0.73942082,\n",
       "         0.74054108,  0.74078839,  0.74588366,  0.74800832,  0.75035997,\n",
       "         0.7505686 ,  0.75756047,  0.75489637,  0.78571341,  0.78807621,\n",
       "         0.78509469,  0.79957159,  0.78999959,  0.78988764,  0.78737088,\n",
       "         0.79306296,  0.79321596,  0.80481676,  0.81191106,  0.80126638,\n",
       "         0.80778397,  0.80747156,  0.80519781,  0.81512166,  0.80563987,\n",
       "         0.80978361,  0.74561036,  0.73886333,  0.7404104 ,  0.74891227,\n",
       "         0.75409379,  0.74869714,  0.75203416,  0.74851918,  0.75431668,\n",
       "         0.78430062,  0.78444427,  0.78548034,  0.79443738,  0.79366696,\n",
       "         0.79215515,  0.79389832,  0.79222823,  0.78820055,  0.80859244,\n",
       "         0.80940372,  0.8129727 ,  0.81263344,  0.80614826,  0.81119595,\n",
       "         0.80600814,  0.8164669 ,  0.80781458]),\n",
       " 'split2_train_score': array([ 0.73137303,  0.73369615,  0.73327701,  0.74626296,  0.74796714,\n",
       "         0.74678954,  0.75198955,  0.75165726,  0.75021695,  0.79908965,\n",
       "         0.79997147,  0.79773965,  0.81313612,  0.81055941,  0.81089543,\n",
       "         0.82013803,  0.8227035 ,  0.81795437,  0.83023563,  0.82746412,\n",
       "         0.82936603,  0.84185949,  0.84422156,  0.83909064,  0.85366903,\n",
       "         0.85197048,  0.84799311,  0.73486891,  0.73302292,  0.73133811,\n",
       "         0.74878975,  0.74350602,  0.74450064,  0.75105076,  0.75164988,\n",
       "         0.75202699,  0.81388382,  0.8134365 ,  0.81267063,  0.82963936,\n",
       "         0.83039404,  0.82730319,  0.83605294,  0.83471856,  0.83408349,\n",
       "         0.85928038,  0.85750762,  0.85440021,  0.86946812,  0.87018426,\n",
       "         0.87080888,  0.88119149,  0.87878501,  0.8777206 ,  0.73389188,\n",
       "         0.73582374,  0.73168184,  0.74240698,  0.74104473,  0.74629832,\n",
       "         0.75060581,  0.75563128,  0.75182264,  0.81522035,  0.81517552,\n",
       "         0.810416  ,  0.83234147,  0.83041101,  0.82725452,  0.8368252 ,\n",
       "         0.83903686,  0.83838071,  0.87222846,  0.86963972,  0.86711657,\n",
       "         0.88250685,  0.88585005,  0.88243787,  0.89309549,  0.89570716,\n",
       "         0.89157945,  0.73464142,  0.73544251,  0.73605181,  0.74584513,\n",
       "         0.74758095,  0.74522047,  0.7518961 ,  0.7533554 ,  0.75467959,\n",
       "         0.81334032,  0.8151098 ,  0.81244559,  0.83051747,  0.83061188,\n",
       "         0.82788889,  0.83999293,  0.84222448,  0.83749889,  0.87375297,\n",
       "         0.87132567,  0.87160404,  0.89649075,  0.8905833 ,  0.88835776,\n",
       "         0.90135193,  0.90226148,  0.8990269 ]),\n",
       " 'std_fit_time': array([ 0.00338457,  0.0009488 ,  0.00170382,  0.01315835,  0.0028293 ,\n",
       "         0.00219781,  0.00513764,  0.00088025,  0.00477241,  0.00140145,\n",
       "         0.00028379,  0.00254897,  0.00110921,  0.00456828,  0.00348399,\n",
       "         0.00352914,  0.00932179,  0.0009958 ,  0.00540319,  0.00553543,\n",
       "         0.00518029,  0.08862848,  0.06979487,  0.04883456,  0.01701663,\n",
       "         0.00304791,  0.00651833,  0.00423761,  0.0037574 ,  0.00359042,\n",
       "         0.01079056,  0.00522035,  0.0091513 ,  0.00518095,  0.00545849,\n",
       "         0.00376381,  0.0050321 ,  0.02320228,  0.01291443,  0.00284673,\n",
       "         0.00930723,  0.00419739,  0.00548822,  0.00630711,  0.0138098 ,\n",
       "         0.00289866,  0.00181442,  0.00416804,  0.00282979,  0.00456949,\n",
       "         0.00928418,  0.00475686,  0.00493782,  0.00652478,  0.00139413,\n",
       "         0.00285872,  0.00470741,  0.05010607,  0.01428158,  0.01225199,\n",
       "         0.00581776,  0.01757503,  0.00385044,  0.00439951,  0.00272171,\n",
       "         0.00185241,  0.00396581,  0.00205563,  0.00068609,  0.00315733,\n",
       "         0.00812222,  0.00201777,  0.00538236,  0.006476  ,  0.00785704,\n",
       "         0.00554701,  0.00695081,  0.00568423,  0.00658096,  0.00155183,\n",
       "         0.00510071,  0.00158157,  0.00625434,  0.0011884 ,  0.0027978 ,\n",
       "         0.00671993,  0.0036393 ,  0.04645173,  0.0185119 ,  0.01272043,\n",
       "         0.00571187,  0.00210374,  0.02407545,  0.01755264,  0.00595993,\n",
       "         0.01752316,  0.02909095,  0.00878546,  0.02670807,  0.02746722,\n",
       "         0.04679801,  0.04032973,  0.04569015,  0.02595627,  0.01096508,\n",
       "         0.03247709,  0.01297969,  0.00540628]),\n",
       " 'std_score_time': array([  3.80380403e-04,   7.35361473e-05,   8.46060913e-05,\n",
       "          2.47574962e-04,   2.92961518e-04,   1.25557410e-04,\n",
       "          1.88324554e-04,   8.58353243e-05,   2.40600011e-04,\n",
       "          2.11702005e-04,   1.92994796e-04,   1.95120991e-03,\n",
       "          3.09004971e-04,   2.99642803e-04,   1.27270618e-04,\n",
       "          3.69373625e-04,   3.66658527e-05,   4.47026182e-05,\n",
       "          1.18926879e-04,   9.45413762e-05,   1.58517630e-03,\n",
       "          1.20868217e-03,   5.57411058e-04,   5.06727978e-04,\n",
       "          3.55164788e-04,   3.01546129e-04,   1.76006352e-04,\n",
       "          1.65617741e-04,   1.10090595e-04,   1.33180769e-04,\n",
       "          2.19203753e-04,   6.20370732e-04,   2.22970056e-04,\n",
       "          2.63624713e-04,   1.90949154e-04,   2.17170346e-04,\n",
       "          1.71931703e-04,   1.71275304e-03,   1.41743303e-03,\n",
       "          5.49426510e-04,   1.19072571e-04,   1.69812209e-04,\n",
       "          2.84340543e-04,   2.60666715e-04,   4.36909894e-04,\n",
       "          2.11342796e-04,   1.60043620e-04,   2.40564937e-04,\n",
       "          2.20970882e-04,   3.42660190e-04,   1.44812723e-04,\n",
       "          1.53681196e-04,   4.36863575e-04,   9.78121402e-05,\n",
       "          1.61057275e-04,   4.39813763e-04,   4.00894647e-04,\n",
       "          3.37240044e-04,   1.67493007e-03,   1.95130844e-04,\n",
       "          3.34431973e-04,   1.11039148e-03,   1.25972480e-04,\n",
       "          4.18481773e-05,   1.67480185e-04,   3.02036601e-05,\n",
       "          3.47475483e-05,   1.23922815e-03,   3.54214931e-04,\n",
       "          3.55906210e-04,   4.32130282e-04,   3.13516795e-04,\n",
       "          2.05874777e-04,   4.64750991e-04,   2.62351839e-04,\n",
       "          5.76164530e-06,   6.00474388e-04,   1.27016367e-03,\n",
       "          5.66825028e-04,   9.34122925e-05,   3.05162556e-04,\n",
       "          2.41021963e-04,   2.15846678e-04,   2.75048047e-04,\n",
       "          1.08499985e-04,   3.06458703e-04,   2.80733667e-04,\n",
       "          2.81880027e-04,   6.32570885e-04,   3.03241628e-04,\n",
       "          4.00715320e-04,   4.34225291e-04,   5.72547854e-04,\n",
       "          3.04783401e-04,   2.19475004e-04,   3.32594649e-04,\n",
       "          6.55632568e-04,   6.63372475e-04,   9.54513049e-05,\n",
       "          9.23363142e-05,   4.66913177e-03,   7.00066646e-04,\n",
       "          5.55843675e-03,   1.32792138e-03,   1.77534293e-04,\n",
       "          2.68078031e-03,   7.28071926e-04,   1.25482990e-04]),\n",
       " 'std_test_score': array([ 0.02505439,  0.02335413,  0.02501724,  0.02738895,  0.03061685,\n",
       "         0.02730205,  0.03011401,  0.03047576,  0.02921346,  0.02337628,\n",
       "         0.022918  ,  0.02273334,  0.02059041,  0.0227089 ,  0.0239412 ,\n",
       "         0.0186376 ,  0.02127763,  0.02456252,  0.03121643,  0.02270434,\n",
       "         0.02112968,  0.02576067,  0.02303063,  0.02433552,  0.02819802,\n",
       "         0.02458167,  0.0263241 ,  0.02862708,  0.0244317 ,  0.02525026,\n",
       "         0.02902732,  0.02511843,  0.03290756,  0.02730172,  0.02963458,\n",
       "         0.02880836,  0.02144702,  0.02184298,  0.02303776,  0.02342027,\n",
       "         0.02076131,  0.02033473,  0.02383075,  0.02228153,  0.02322595,\n",
       "         0.02267655,  0.02428732,  0.02269402,  0.02451978,  0.02396736,\n",
       "         0.02252139,  0.02242465,  0.02102358,  0.02514643,  0.0270914 ,\n",
       "         0.0266393 ,  0.02626902,  0.02609681,  0.02854176,  0.02960717,\n",
       "         0.02750479,  0.03227781,  0.03028391,  0.02031369,  0.01978736,\n",
       "         0.02344074,  0.02373902,  0.02219282,  0.02627335,  0.01832794,\n",
       "         0.02361959,  0.01919912,  0.01984741,  0.02659073,  0.02090416,\n",
       "         0.02175452,  0.01959717,  0.02210217,  0.03167289,  0.02335611,\n",
       "         0.0247467 ,  0.0290986 ,  0.02597851,  0.02792064,  0.02896389,\n",
       "         0.03073049,  0.02826548,  0.0311755 ,  0.02717822,  0.03036179,\n",
       "         0.01740508,  0.02160356,  0.02039315,  0.02222664,  0.02352114,\n",
       "         0.02072113,  0.02059494,  0.02285052,  0.02098444,  0.02479094,\n",
       "         0.02462516,  0.02586952,  0.02529957,  0.0244951 ,  0.02757734,\n",
       "         0.02418086,  0.02608962,  0.02519334]),\n",
       " 'std_train_score': array([ 0.01277819,  0.01297162,  0.01124635,  0.01172167,  0.01247499,\n",
       "         0.01142671,  0.0119316 ,  0.01248105,  0.01289198,  0.00944307,\n",
       "         0.00917805,  0.00974165,  0.00946153,  0.00924254,  0.00972677,\n",
       "         0.00817375,  0.00844298,  0.00979061,  0.00896897,  0.01218392,\n",
       "         0.00973582,  0.01040276,  0.00886628,  0.01048439,  0.00902017,\n",
       "         0.00835839,  0.00977337,  0.0102576 ,  0.01376051,  0.01234943,\n",
       "         0.01127542,  0.01539214,  0.01216486,  0.01379077,  0.01296424,\n",
       "         0.01256224,  0.00857102,  0.00855845,  0.00923032,  0.00798542,\n",
       "         0.00873334,  0.0081208 ,  0.00959662,  0.00949063,  0.0088275 ,\n",
       "         0.00757593,  0.00853541,  0.00841912,  0.01171142,  0.00896982,\n",
       "         0.00831319,  0.00836659,  0.00935178,  0.00813424,  0.01271777,\n",
       "         0.01034167,  0.012693  ,  0.01424543,  0.01454296,  0.01188641,\n",
       "         0.01437813,  0.01367033,  0.01286734,  0.00951954,  0.00930201,\n",
       "         0.01055331,  0.00808944,  0.00949373,  0.00999481,  0.0095518 ,\n",
       "         0.00876938,  0.00784516,  0.00731995,  0.0080586 ,  0.00727567,\n",
       "         0.00878307,  0.00881363,  0.00913317,  0.00841975,  0.00754021,\n",
       "         0.0086958 ,  0.01282928,  0.01114195,  0.00839947,  0.01205593,\n",
       "         0.01070678,  0.0128363 ,  0.01221093,  0.01441714,  0.01170172,\n",
       "         0.00962499,  0.00797376,  0.01013283,  0.00841697,  0.00920343,\n",
       "         0.00946813,  0.00677244,  0.00655768,  0.00816366,  0.00861706,\n",
       "         0.00881662,  0.00864438,  0.00439595,  0.00728449,  0.00895041,\n",
       "         0.00728237,  0.00654   ,  0.00760656])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.05,\n",
       "             n_estimators=120, presort='auto', random_state=None,\n",
       "             subsample=0.6, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Selección\n",
    "\n",
    "Genial, ya sabemos que parametros hemos de coger y que son lo más optimos en este caso, veamos que se obtiene ahora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14901905  0.13464619  0.12518018  0.15877724  0.13403575  0.14804185\n",
      "  0.13385177  0.12829163  0.13793748  0.12125371]\n",
      "[ 0.67796801  0.79787778  0.76286809  0.69742384  0.81480704  0.75726779\n",
      "  0.78034657  0.80347735  0.77701473  0.82469473]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def score_dataset_cv(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor(n_estimators= 100, max_depth=5, subsample= 0.7\n",
    "                                      ,min_weight_fraction_leaf=0.1)\n",
    "    scores = cross_validate(model, X_train, y_train,\n",
    "                         scoring=('r2', 'neg_mean_absolute_error'), cv=10)\n",
    "    print(-scores['test_neg_mean_absolute_error'])      \n",
    "    print(scores['test_r2'])                         \n",
    "\n",
    "score_dataset_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y en este caso el error es: \n",
      "0.11875604746\n"
     ]
    }
   ],
   "source": [
    "#Error cometido en esta medicion MAE \n",
    "prediccion = clf.predict(X_test)\n",
    "print(\"y en este caso el error es: \")\n",
    "print (mean_absolute_error(y_test,prediccion ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQwREUnl/wPWWnUrEZL3zQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f340fa16240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veamoslo en un scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(prediccion, y_test);\n",
    "plt.title('Validación');\n",
    "plt.ylabel('Modelo');\n",
    "plt.xlabel('Prediccion');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
