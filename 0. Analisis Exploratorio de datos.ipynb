{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis exploratorio de datos\n",
    "Basado en Hair et AI. (2013), capítulo 'Examining your data', vamos a proceder con los siguientes pasos\n",
    "\n",
    "* **1. Entender el problema:** Mirar cada variable y su relevancia para resolver el problema\n",
    "* **2. Análisis univariante:** realizado sobre la variable target (SalesPrice)\n",
    "* **3. Análisis multiunivariante:** para obtener variables dependientes e independientes\n",
    "* **4. Limpieza de datos:** detectar NAs, outliers y variables categóricas\n",
    "* **5. Transformación de datos:** aplicando análisis multivariante, vamos a tener que \n",
    "    - Normalizar datos: hacer que sigan una distribucióon normal (dado que luego cuando usemos algunos análisis estadísticos, si no lo siguen nos saldrán mal). Lo haremos solo respecto a la variable target y si tenemos pocos datos, dado que este punto para grandes volumenes de datos no suele ser un problema\n",
    "    - Heterocedasticidad: para asegurar que un error no es constante para todas las variables independientes\n",
    "    - Linealidad\n",
    "    - Asegurar que no hay errores correlacionados\n",
    "\n",
    "* **6. Conclusiones** \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1. Entender el problema\n",
    "\n",
    "Realizamos Importación de librerías y ficheros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Importación de ficheros\n",
    "df_train = pd.read_csv('data/PreciosCasas/train.csv')\n",
    "df_train.describe();\n",
    "df_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, hemos importado 1460 registros distintos, y disponemos de varias variables para determinar el precio de las casas\n",
    "Tenemos de variables numéricas y categóricas, además, para empezar el análisis vamos a:\n",
    "* seleccionar aquellas que por lógica, nos parece que guardarán más relación con la variable target (SalesPrice). Por ejemplo, si el tener o no garaje puede encarecer un piso (que parece indicar que si), o\n",
    "* y que variables a priori, parecen relacionadas entre sí. Por ejemplo, hasta que punto necesito LandScope, que me indica ya la inclinación de la propiedad, si tengo ya LandContour\n",
    "\n",
    "A priori, suponemos que las siguientes variables son importantes: OverallQual, YearBuilt, TotalBsmtSF, GrLivArea, Neigborhood\n",
    "\n",
    "y analicemos entonces la **variable target**. Se ve el los resultados anteriores ( o por ``df_train['SalePrice'].describe()`` ) que sí tenemos datos para las 1460 variables, que el mínimo es mayor que cero (y por lo tanto, sí hay información) y que además se distribuye siguiendo una distribución normal con asimetría positiva (possitive skewness), es decir, son sesgo en valores superiores a la media (tenemos propiedades cuyo precio es muy superior al de la mayoría), y con los valores concentrados en la región central de la distribución (curtosis > D. normal). \n",
    "**Podemos por tanto continuar el análisis para predicción de esa variable **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograma\n",
    "sns.distplot(df_train['SalePrice']);\n",
    "\n",
    "# Valor de la asimetría y curtosis\n",
    "print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis univariante \n",
    "¿Qué relación hay entre el target y el resto de variables?\n",
    "\n",
    "Hemos dicho que íbamos a seleccionar ciertas variables, veamos como se relacionan con la target\n",
    "\n",
    "***Variables numéricas***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) scatter plot entre grlivarea/saleprice\n",
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n",
    "\n",
    "# 2) scatter plot entre totalbsmtsf/saleprice\n",
    "var = 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n",
    "\n",
    "#3) Vecindario \n",
    "var= 'Neighborhood'\n",
    "plt.figure(figsize = (12, 6))\n",
    "sns.countplot(x =var, data = df_train)\n",
    "xt = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parece haber una relación lineal con grlivarea y menos lineal con totalbsmtsf (de hecho, lineal hasta cierto punto y luego parece exponencial)\n",
    "\n",
    "***Variables Categóricas***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) box plot overallqual/saleprice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "\n",
    "#2) box plot YearBuilt/saleprice\n",
    "var = 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.xticks(rotation=90);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "De nuevo sí, hay relación entre OverallQual, la calidad de la casa y el precio, por un lado, y sorprendentemente, no tan lineal con el YearBuilt, pues parece que un poco sí cuestan más las casas más nuevas... pero no del todo. Aquí sería interesante saber hasta que punto el efecto de la inflación se ha tenido en cuenta y cuan comparables son los precios a lo largo de los años, en cualqueir caso, para eso tenemos el año de venta y se analizará en el multivariante\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis multivariante\n",
    "Las variables anteriores se han seleccionado por lógiva, pero no podemos estar seguros de que son todas correctas, lo óptimo es establecer una matriz de correlación a ver que variables resultan significativas, cuales no y su nivel de relación con la target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=1, cmap=\"YlGnBu\" , square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama la atención los dos cuadrados más claritos, los que relacionan 'TotalBsmtSF' y '1stFlrSF' y todos los de 'Garage...' , y claro, tiene sentido, seguramente es que están correlacionadas (y no debemos usarlas a la vez, pues proporcionarán luego \"ruido\" al modelo).\n",
    "\n",
    "\n",
    "Por otro lado, analizando la línea de la target, SalesPrice, vemos que en efecto, las variables antes seleccionadas por lógica 'GrLivArea', 'TotalBsmtSF' y 'OverallQual', están bien correlacionadas con la target.\n",
    "Contabilicemos cuanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice matrix de correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, cmap=\"YlGnBu\" , annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_dict = corrmat['SalePrice'].to_dict()\n",
    "del cor_dict['SalePrice']\n",
    "print(\"Correlación descendiente TodasVbles vs Sale Price:\\n\")\n",
    "for ele in sorted(cor_dict.items(), key = lambda x: -abs(x[1])):\n",
    "    print(\"{0}: \\t{1}\".format(*ele))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos estas variables\n",
    "- 'OverallQual'\n",
    "- 'GrLivArea'\n",
    "- 'GarageCars' y 'GarageArea' también, pero como están correlacionadas entre ellas, elegimos solo 'GarageCars' (la de correlación mayor).\n",
    "- 'TotalBsmtSF' y '1stFloor' Lo mismo. Seleccionamos 'TotalBsmtSF'\n",
    "- 'FullBath'\n",
    "- 'TotRmsAbvGrd' y 'GrLivArea' Lo mismo. Seleccionamos 'GrLivArea'\n",
    "- 'YearBuilt' también, aunque hemos visto antes que podía ser rara\n",
    "\n",
    "Vamos a ver su distribución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'OverallQual', y = 'SalePrice', data = df_train, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(df_train[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que 'TotalBsmtSF' y 'GrLiveArea' dibujan prácticamente una linea y el resto de puntos van por debajo... claro, puedes tener garajes que ocupen toda la propiedad pero no al revés (sería absurdo)\n",
    "Con 'SalePrice' y 'YearBuilt' pasa lo que adelantábamos antes, a partir de cierto año los precios crecen mucho, en cualqier caso, sigue siendo una variable a tener en cuenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Limpieza de datos\n",
    "\n",
    "- **Missing data**\n",
    "\n",
    "    Este tema es muy relevante, pues ¿hasta que punto los NAs siguen un patrón aleatorio o son constantes en nuestros datos? Puede llevarnos a reducir tanto el tamaño de la muestra que hasta nos impida hacer el análisis. Pasemos pues a analizarlo y ver hasta que punto las variables que hemos visto más significativas, están completas o no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data ordenados por %\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hipótesis: un dato no es correcto si tiene más de un 15 % de NAs*\n",
    "\n",
    "Según esto\n",
    "- 'PoolQC', 'MiscFeature', 'Alley', etc no podremos considerarlas (pero no importa, no parecían relevantes tampoco, e incluso ser outliers)\n",
    "- 'Garage..' lo mismo, pero como 'GarageCars' no está en la lista, sí (tendrá como máximo un 5% de los valores NAs, pero no más)\n",
    "- 'MasVnrArea' y 'MasVnrType' tampoco parecían relevantes, así que bien (y mirando el mapa de calor, estabas correlacionadas con 'YearBuilt' y 'OverallQual' así que no perdemos nada)\n",
    "- 'Electrical' tiene solo una observación, así que la borraremos pero sí mantendremos la variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creación de los nuevo data frames\n",
    "df_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n",
    "\n",
    "# Veamos que está bien (debe dar 0\n",
    "df_train.isnull().sum().max() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Outliars**\n",
    "\n",
    "    Hay que definir un umbral a partir del cual definimos un dato como outliar, para lo cual necesitaremos estandarizar los datos, y analizar cuanto se desvian de 0 por encima y por debajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estandarizar datos del target\n",
    "saleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\n",
    "low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n",
    "high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n",
    "print('Rango bajo (low) de la distribución:')\n",
    "print(low_range)\n",
    "print('\\nRango alto (high) de la distribución')\n",
    "print(high_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los valores por debajo no preocupan (en torno a -1), pero los por encima si, especiamente esos de más de 7 (seguramente son outliars). Veamos estos datos en perspectiva con las dos variables más significativas o relacionadas con la target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saleprice/grlivarea\n",
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en efecto, tenemos dos valores que para ese tamaño de propiedad, no pega que sean tan bajos y se desvian de la tendencia, así que los quitaremos. Mantendremos sin embargo los dos valores de precio más alto, dado que SI parecen mantener esa tendencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Eliminar outliers\n",
    "df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 524].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saleprice/TotalBsmtSF\n",
    "var = 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso, no merece la pena quitar nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saleprice/OverallQual\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformación de datos\n",
    "\n",
    "Veamos como han quedado los datos y que distribución siguen para saber si debemos ajustarlos de alguna manera\n",
    "\n",
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograma y normal probability plot\n",
    "sns.distplot(df_train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para casos de asimetría positiva, se puede lograr que los datos sigan una distribución normal mediante una transformación logartimica, esto es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación logarítmica\n",
    "df_train_log = pd.DataFrame(df_train)\n",
    "df_train_log ['SalePrice'] = np.log(df_train['SalePrice'])\n",
    "\n",
    "# nuevo histograma y normal probability plot\n",
    "sns.distplot(df_train_log['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_log['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y claro, pasará lo mismo con \n",
    "\n",
    "    1) 'GrLivArea'\n",
    "    2) 'TotalBsmtSF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) GrLivArea\n",
    "#histograma y normal probability plot\n",
    "sns.distplot(df_train_log['GrLivArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_log['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación logaritmica\n",
    "df_train_log ['GrLivArea'] = np.log(df_train_log['GrLivArea'])\n",
    "\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train_log['GrLivArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_log['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) TotalBsmtSF\n",
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train_log['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_log['TotalBsmtSF'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hay valores = 0, por lo tanto NO se puede aplicar logaritmos. De nuevo, hay que hacer una suposición, cuando esos ceros seguramente se refieren a que no hay sotano, así que en este caso, la teoría dice que hay que hacer una nueva variable binaria (siene sotano si/no) y luego a los que sí, aplicar ya la transformación de los datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_log['ConBasement'] = 1\n",
    "df_train_log['ConBasement'][df_train_log['TotalBsmtSF'] ==0] = int (0)\n",
    "\n",
    "# Y ahora quitamos los 0 de TotalBsmtSF y hacemos el logaritmo\n",
    "df_train_log2 = pd.DataFrame(df_train_log)\n",
    "\n",
    "# transformación logaritmica\n",
    "df_train_log2 ['TotalBsmtSF'][df_train_log2 ['TotalBsmtSF']!= 0] = np.log1p(df_train_log2['TotalBsmtSF'])\n",
    "\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train_log2['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_log2['TotalBsmtSF'], plot=plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Heterocedasticidad\n",
    "Lo haremos de forma gráfica: si tiene forma cónica o de diamante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "plt.scatter(df_train['GrLivArea'], df_train['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train_log['GrLivArea'], df_train_log['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes los datos tenian forma de diamante, pero tras la normalización ya no, por lo que no tenemos problemas en este punto para 'GrLivArea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "plt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "plt.scatter(df_train_log[df_train_log['TotalBsmtSF']>0]['TotalBsmtSF'], df_train_log[df_train_log['TotalBsmtSF']>0]['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en este caso igual, parece que la varianza del target respecto a TotalBsmtSF es similar en todo el rango"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Dummy\n",
    "Hay una función que lo hace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert categorical variable into dummy\n",
    "df_train_log = pd.get_dummies(df_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "Disponemos de un data frame limpio y listo para ser usado en los siguientes apartados de modelización. Subámoslo pues a nuestro repositorio para poder usarlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_log.to_csv('data/PreciosCasas/train_final.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
